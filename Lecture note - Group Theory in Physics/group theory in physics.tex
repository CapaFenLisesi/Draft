% The entire content of this work (including the source code
% for TeX files and the generated PDF documents) by 
% Hongxiang Chen (nicknamed we.taper, or just Taper) is
% licensed under a 
% Creative Commons Attribution-NonCommercial-ShareAlike 4.0 
% International License (Link to the complete license text:
% http://creativecommons.org/licenses/by-nc-sa/4.0/).
\documentclass{article}

\usepackage{float}  % For H in figures
\usepackage{amsmath} % For math
\usepackage{amssymb}
\usepackage{mathrsfs}
% Followings are for the special character: differential "d".
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\numberwithin{equation}{subsection} % have the enumeration go to the subsection level.
                                    % See:https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics
\usepackage{graphicx}   % need for figures
\usepackage{cite} % need for bibligraphy.
\usepackage[unicode]{hyperref}  % make every cite a link
\usepackage{CJKutf8} % For Chinese characters
\usepackage{fancyref} % For easy adding figure,equation etc in reference. Use \fref or \Fref instead of \ref

% Following is for theorems etc environments
% http://tex.stackexchange.com/questions/45817/theorem-definition-lemma-problem-numbering && https://en.wikibooks.org/wiki/LaTeX/Theorems
\usepackage{amsthm}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{coro}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{defi}{Definition}[section]
\newtheorem{ex}{Example}[section]
\newtheorem{fact}{Fact}[section]

\usepackage{CJKutf8} % For chinese characters.
% A list of nomenclatures.
\usepackage{nomencl}
\makenomenclature

% For highlighting
\usepackage{xcolor,soul}
\newcommand{\hlMath}[1]{\colorbox{yellow!80}{$\displaystyle#1$}}

% My own physics package
% The following line load the package xparse with additional option to
% prevent the annoying warnings, which are caused by the package
% "physics" loaded in package "physics-taper".
\usepackage[log-declarations=false]{xparse}
\usepackage{physics-taper}

\title{Group Theory in Physics (Course Note)}
\date{\today}
\author{Taper}


\begin{document}


\maketitle
\abstract{
This is our course note for the course about group theoy, with its
application in physics.
}
\tableofcontents
\begin{enumerate}
    \item Combine group theory with you research.
    \item Mid-term and final.
    \item No homework, cause it is already graduate level.
    \item Professor Fei Ye. (Phone: 88018229, 228 in Research building 2), 
        T.A. Zhe Zhang. (110 Research building 2).
\end{enumerate}

\section{20160919}
\label{sec:20160919}

He first introduces several common examples of symmetries in our life and
physics. Omitted, with one exception:

He mentions that there is one more symmetry in the Hydrogen 
Hamiltonian: the Laplace-Runge-Lenz symmetry. (So its symmetry 
group is not just $SO(3)$, but two copies of $SO(3)$ that forms a $SO(4)$.
And using the representation of $SO(4)$, the complete spectrum of
Hydrogen Hamiltonian is solved. Hence this $SO(4)$ is the largest
symmetry of Hydrogen Hamiltonian.

    \subsection{Digression about Lenz vector}
    \label{sec:Digression_about_Lenz_vector}
    Since the class is too boring, I checked about the Lenz vector via
    Google and found this Math.SE question \cite{math.se_1_lenz_vector}

    The first answer to that post is:

    \begin{center}\noindent\rule{8cm}{0.4pt}\end{center}

    \begin{quote}
        1) \textbf{Problem}. The \href{http://en.wikipedia.org/wiki/Kepler_problem}{Kepler Problem} has Hamiltonian

        $$ H~:=~ \frac{p^2}{2m}- \frac{k}{q},  $$

        where $m$ is the 2-body reduced mass. The [Laplace–Runge–Lenz vector](http://en.wikipedia.org/wiki/Laplace%E2%80%93Runge%E2%80%93Lenz_vector) is (up to an irrelevant normalization)

        $$ A^j ~:=~a^j  + km\frac{q^j}{q}, \qquad a^j~:=~({\bf L} \times {\bf p})^j~=~{\bf q}\cdot{\bf p}~p^j- p^2~q^j,\qquad {\bf L}~:=~ {\bf q} \times {\bf p}.$$ 

        2) \textbf{Action}. The Hamiltonian Lagrangian is

        $$  L_H~:=~ \dot{\bf q}\cdot{\bf p} - H, $$

        and the action is 

        $$ S[{\bf q},{\bf p}]~=~ \int {\rm d}t~L_H .$$

        The non-zero fundamental canonical Poisson brackets are 

        $$ \{ q^i , p^j\}~=~ \delta^{ij}. $$

        3) \textbf{Inverse Noether's Theorem}. Quite generally in the Hamiltonian formulation, given a constant of motion $Q$, then the infinitesimal variation 

        $$\delta~=~ \varepsilon \{Q,\cdot\}$$ 

        is a global off-shell symmetry of the action $S$ (modulo boundary terms). Here $\varepsilon$ is an infinitesimal global parameter, and $X_Q=\{Q,\cdot\}$ is a Hamiltonian vector field with Hamiltonian generator $Q$. The full Noether current is (minus) $Q$, see e.g. my answer to [this question](http://physics.stackexchange.com/q/8626/2451). \hl{(The words \textbf{on-shell}. and \textbf{off-shell}. refer to whether the equations of motion are satisfied or not.)}

        4) \textbf{Variation}. Let us check that the three Laplace–Runge–Lenz components $A^j$ are Hamiltonian generators of three continuous global off-shell symmetries of the action $S$. In detail, the infinitesimal variations $\delta= \varepsilon_j \{A^j,\cdot\}$ read

        $$ \delta  q^i ~=~  \varepsilon_j  \{A^j,q^i\}   , \qquad 
         \{A^j,q^i\} ~ =~ 2 p^i q^j - q^i p^j - {\bf q}\cdot{\bf p}~\delta^{ij}, $$
        $$ \delta  p^i ~=~  \varepsilon_j  \{A^j,p^i\}   , \qquad  
        \{A^j,p^i\}~ =~ p^i p^j - p^2~\delta^{ij} +km\left(\frac{\delta^{ij}}{q}- \frac{q^i q^j}{q^3}\right), $$
        $$ \delta  t ~=~0,$$

        where $\varepsilon_j$ are three infinitesimal parameters.

        5) Notice for later that

        $$ {\bf q}\cdot\delta {\bf q}~=~\varepsilon_j({\bf q}\cdot{\bf p}~q^j - q^2~p^j),  $$

        $$ {\bf p}\cdot\delta {\bf p}
        ~=~\varepsilon_j km(\frac{p^j}{q}-\frac{{\bf q}\cdot{\bf p}~q^j}{q^3})~=~- \frac{km}{q^3}{\bf q}\cdot\delta {\bf q},  $$

        $$ {\bf q}\cdot\delta {\bf p}~=~\varepsilon_j({\bf q}\cdot{\bf p}~p^j - p^2~q^j )~=~\varepsilon_j a^j,  $$

        $$ {\bf p}\cdot\delta {\bf q}~=~2\varepsilon_j( p^2~q^j - {\bf q}\cdot{\bf p}~p^j)~=~-2\varepsilon_j a^j~.  $$

        6) The Hamiltonian is invariant

        $$ \delta  H ~=~ \frac{1}{m}{\bf p}\cdot\delta {\bf p} + \frac{k}{q^3}{\bf q}\cdot\delta {\bf q}~=~0, $$

        showing that the Laplace–Runge–Lenz vector $A^j$ is classically a constant of motion 

        $$\frac{dA^j}{dt} ~\approx~ \{ A^j, H\}+\frac{\partial A^j}{\partial t} ~=~  0.$$   

        (We will use the $\approx$ sign to stress that an equation is an on-shell equation.) 

        7) The variation of the Hamiltonian Lagrangian $L_H$ is a total time derivative

        $$ \delta L_H~=~ \delta  (\dot{\bf q}\cdot{\bf p})~=~ \dot{\bf q}\cdot\delta {\bf p} - \dot{\bf p}\cdot\delta {\bf q} + \frac{d({\bf p}\cdot\delta {\bf q})}{dt} $$
        $$  =~ \varepsilon_j\left( \dot{\bf q}\cdot{\bf p}~p^j - p^2~\dot{q}^j +  km\left( \frac{\dot{q}^j}{q} -  \frac{{\bf q} \cdot \dot{\bf q}~q^j}{q^3}\right)\right)    $$
        $$- \varepsilon_j\left(2 \dot{\bf p}\cdot{\bf p}~q^j - \dot{\bf p}\cdot{\bf q}~p^j- {\bf p}\cdot{\bf q}~\dot{p}^j  \right) - 2\varepsilon_j\frac{da^j}{dt}$$
        $$ =~\varepsilon_j\frac{df^j}{dt}, \qquad f^j ~:=~ A^j-2a^j, $$

        and hence the action $S$ is invariant off-shell up to boundary terms.

        8) \textbf{Noether current}. The bare \href{http://en.wikipedia.org/wiki/Noether%27s_theorem}{Noether current} $j^k$ is

        $$j^k~:=~ \frac{\partial L_H}{\partial \dot{q}^i}  \{A^k,q^i\}+\frac{\partial L_H}{\partial \dot{p}^i}  \{A^k,p^i\}
        ~=~ p^i\{A^k,q^i\}~=~ -2a^k. $$

        The full Noether current $J^k$ (which takes the total time-derivative into account) becomes (minus) the Laplace–Runge–Lenz vector

        $$ J^k~:=~j^k-f^k~=~ -2a^k-(A^k-2a^k)~=~ -A^k.$$

        $J^k$ is conserved on-shell 

        $$\frac{dJ^k}{dt} ~\approx~  0,$$  

        due to \href{http://en.wikipedia.org/wiki/Noether%27s_theorem}{Noether's first Theorem}. Here $k$ is an index that labels the three symmetries.
    \end{quote}

    \begin{center}\noindent\rule{8cm}{0.4pt}\end{center}

    However, I don't really understand the content inside. I asked professor
    Ye whether we can find some physics about this conserved quantity, and
    he answered with no.

    The next answer is also interesting:
    
    \begin{center}\noindent\rule{8cm}{0.4pt}\end{center}

    \begin{quote}
        While Kepler second law is simply a statement of the conservation of angular momentum (and as such it holds for all systems described by central forces), \hl{the first and the third laws are special and are linked with the unique form of the newtonian potential $-k/r$.} In particular, \hl{Bertrand theorem assures that *only* the newtonian potential and the harmonic potential $kr^2$ give rise to closed orbits (no precession).} It is natural to think that this must be due to some kind of symmetry of the problem. In fact, the particular symmetry of the newtonian potential is described exactly by the conservation of the RL vector (\hl{it can be shown that the RL vector is conserved iff the potential is central and newtonian}). This, in turn, is due to a more general symmetry: \hl{if conservation of angular momentum is linked to the group of special orthogonal transformations in 3-dimensional space $SO(3)$, conservation of the RL vector must be linked to a 6-dimensional group of symmetries}, since in this case there are apparently six conserved quantities (3 components of $L$ and 3 components of $\mathcal A$). In the case of bound orbits, this group is $SO(4)$, the group of rotations in 4-dimensional space.  

        Just to fix the notation, the RL vector is:

        \begin{equation} \mathcal{A}=\textbf{p}\times\textbf{L}-\frac{km}{r}\textbf{x} \end{equation}

        Calculate its total derivative:

        \begin{equation}\frac{d\mathcal{A}}{dt}=-\nabla U\times(\textbf{x}\times\textbf{p})+\textbf{p}\times\frac{d\textbf{L}}{dt}-\frac{k\textbf{p}}{r}+\frac{km(\textbf{p}\cdot \textbf{x})}{r^3}\textbf{x} \end{equation}

        Make use of Levi-Civita symbol to develop the cross terms:

        \begin{equation}\epsilon_{sjk}\epsilon_{sil}=\delta_{ji}\delta_{kl}-\delta_{jl}\delta_{ki}   \end{equation}

        Finally:

        \begin{equation}
        \frac{d\mathcal{A}}{dt}=\left(\textbf{x}\cdot\nabla U-\frac{k}{r}\right)\textbf{p}+\left[(\textbf{p}\cdot\textbf{x})\frac{k}{r^3}-2\textbf{p}\cdot\nabla U\right]\textbf{x}+(\textbf{p}\cdot\textbf{x})\nabla U
        \end{equation}

        Now, if the potential $U=U(r)$ is central:

        \begin{equation}
        (\nabla U)_j=\frac{\partial U}{\partial x_j}=\frac{dU}{dr}\frac{\partial r}{\partial x_j}=\frac{dU}{dr}\frac{x_j}{r}
        \end{equation}

        so 

        \begin{equation} \nabla U=\frac{dU}{dr}\frac{\textbf{x}}{r}\end{equation}

        Substituting back:

        \begin{equation}
            \hlMath{\frac{d\mathcal A}{dt}=\frac{1}{r}\left(\frac{dU}{dr}-\frac{k}{r^2}\right)[r^2\textbf{p}-(\textbf{x}\cdot\textbf{p})\textbf{x}]}
        \end{equation}

        Now, you see that if $U$ has \textit{exactly} the newtonian form then the first parenthesis is zero and so the RL vector is conserved. 

        Maybe there's some slicker way to see it (Poisson brackets?), but this works anyway.
    \end{quote}
    \begin{center}\noindent\rule{8cm}{0.4pt}\end{center}
    
    % TODO Study in detail about this example.

    \subsection{Coming back to the course}

After mentioning the Poinc\'{a}re group, he produces to review some
concepts about linear algebra:
\begin{enumerate}
    \item The axioms of linear space, using quantum mechanics
        as basic example (Omitted).
    \item Some common concepts of linear space: linear-independence,
        subspace, direct sum, linear operators, its matrix representation. (Omitted)
    \item Introducing the complete antisymmetric tensor 
        $\epsilon^{a_1,\cdots,a_n}$. Some properties:
        \begin{align}
            \frac{1}{(m-n)!} \sum_{a_{n+1},\cdots, a_m}
            & \epsilon_{a_1,\cdots,a_n,a_{n+1},a_m}
            \epsilon_{b_1,\cdots,b_n,a_{n+1},a_m}\nonumber
            \\
            &= \sum_{p_1,\cdots,p_n} 
            \epsilon_{p_1,\cdots,p_n} 
                \delta_{a_1,b_{p_1}}\cdots \delta_{a_n,b_{p_n}}
                \\
            \epsilon_{ab}\epsilon_{rs} &=
                \delta_{ar}\delta_{bs}-\delta_{as}\delta_{br}
                \\
            \sum_d\epsilon_{abd}\epsilon_{rsd} &=
                \delta_{ar}\delta_{bs} + \delta_{as}\delta_{br}
        \end{align}
    \item Some special matrices.
    \item Fact: If $R\Gamma = \Gamma R$, and 
        $\Gamma$ is diagonal. (let $\mu\neq\nu$) Then if 
        $\Gamma_{\mu\mu} \neq \Gamma_{\nu\nu}$ , we have:
        $ R_{\mu\nu}=R_{\nu\mu} = 0 $.
        On the other hand, if $R_{\mu\nu}\neq 0$, then
        $\Gamma_{\mu\mu}=\Gamma_{\nu\nu}$.
        This is obviously from:
        \begin{align*}
            \sum_j R^{i}_{j}\Gamma^{j}_{k}=\sum_j\Gamma^{i}_{j} R^{j}_k
            \Longrightarrow
            R^i_k\Gamma^k_k = \Gamma^i_i R^i_k
        \end{align*}
        where the first is automatically summed, and the second is not.

    \item A linear functional is closed w.r.t. a vector space. (Omitted)
    \item ... then this linear functional can be expressed as a
        matrix w.r.t to a basis of this vector space. (Omitted)
    \item Invariant subspace. (Omitted)
    \item Transformation of basis. (Omitted)
    \item Direct sum of operators:

        Let vector spaces $L=L_1\oplus L_2$, with $L=\braket{e_i}$,
        $L_1=\braket{e'_1,\cdots e'_n}$,$L_2=\braket{e'_{n+1},\cdots,e'_m}$,
        $e'_\nu=\sum_\mu e_\mu  S_{\mu\nu}$. 
        Assume that $L_1,L_2$ are invariant w.r.t $A$, an linear operator. If:
        \begin{align}
            A e'_\mu = \sum_{\nu=1}^{m} e'_\nu R'_{\nu\mu}
        \end{align}
        we have obviously:
        \begin{align}
            A e'_\mu = \sum_{\nu=1}^{n} e'_\nu R'_{\nu\mu} \text{for }
                \mu\in \{1\cdots n\}
                \\
            A e'_\mu = \sum_{\nu=n}^{m} e'_\nu R'_{\nu\mu} \text{for }
                \mu\in \{n\cdots m\}
        \end{align}
        i.e., $A$'s matrix representation has two diagonal blocks.
        Using this fact, $A$ after a linear transformation (by $S$),
        could be written as $R_1\oplus R_2$, where the meaning of $R_1/R_2$
        is obvious.
    \item Eigenvalues and the characteristic equation. (Omitted)
        Some properties:
        \begin{enumerate}
            \item Trace = $\sum_i \lambda_i$
            \item Determinant = $\prod_i \lambda_i$
            \item Geometric multiplicity $\leq$ Algebraic multiplicity, or
                $$\mathrm{dim}V_{\lambda_1} \leq n_1$$.
        \end{enumerate}
    \item Inner product and orthonormal basis. (Omitted) Here we define
        matrix $\Omega$ to be, when a basis $\{e_i\}$is given:
        \begin{defi}[]
        \nomenclature{}{\nomrefpage.}
            \begin{align}
                \Omega_{ij} \equiv \braket{e_i,e_j}
            \end{align}
        \end{defi}
    \item Adjoint operator:

        Let $A$ be a linear operator represented by matrix $A^i_j$. Let
        its adjoint $A^\dagger$ be represented by $R^i_J$. Then using
        $\braket{A^\dagger e_j, e_i} = \braket{e_j,A e_i}$, we will
        get $(R^{k}_j)^* \Omega_{ki}= \Omega_{jk}A^k_i$, i.e.
        $(R^T)^* \Omega = \Omega A$, so:
        \begin{align}
            R = \Omega^{-1} A^\dagger \Omega
        \end{align}
        where we have used the fact that $\Omega^\dagger=\Omega$.

        Note that $(R^{k}_j)^* \Omega_{ki}$ is not $\Omega^T R^*$.
        (Be careful and you will find out why.)

        This is very different from my previous naive concept
        when $\Omega$ is not identity matrix, i.e. when the basis
        is not orthonormal.
\end{enumerate}

\section{20160926}
\label{sec:20160926}

He first introduces some important matrices:

\paragraph{Unitary matrix} Eigenvalues of Unitary matrices has modulus $1$,
i.e. $|\lambda|=1$. This can be proved directly. Also, Unitary matrices are unitarily diagonalizable. This is a result of the following Spectral Theorem:
\begin{thm}[Spectral Theorem]
    A matrix $A$, which is normal (i.e. $A^\dagger A= AA^\dagger$), if and only if it is unitarily diagonalizable.
\end{thm}
\begin{proof}
    If $A$ is normal, then by Schur decomposition, we can write
    $A=UTU^\dagger$, here $U$ is unitary and $T$ is upper-triangular. 
    Using the condition of being normal, one can show directly that $T$ 
    is in fact also normal. Now we show that any triangular matrix that 
    is normal must be diagonal. Observe that we have 
    $\braket{e_i,T^\dagger T e_i}=\braket{e_i,TT^\dagger e_i}$, 
    i.e. $\braket{T^\dagger e_i, T^\dagger e_i}=\braket{T e_i, T e_i}$. 
    This is sayin g that the norm of the first column of $A^\dagger$ 
    is equal to the norm of the first column of $A$. Obviously 
    $A$ has to be diagonal.

    The converse is obvious.
\end{proof}

Also, unitary matrix's eigenvector corresponding to different eigenvalues 
are orthogonal. This is a direct result of fact mentioned above.

\paragraph{Hermitian matrices} They have real eigenvalues and orthogonal
eigenvectors (proof omitted). Also, if $\text{det}(R^\dagger R)\neq 0$,
then $R^\dagger R >0$, i.e. it is positive-definite. 

\textbf{This is wong:} An example is that
the matrix $\Omega$ introduced in the previous lecture has
$\text{det}(\Omega^\dagger \Omega)=\text{det}(\Omega)$, hence
$\text{det}(\Omega)=1$ (it cannot be $0$), hence it is positive definite.

\textbf{Actually} 
$\text{det}(\Omega^\dagger \Omega)\neq\text{det}(\Omega)$, because
\begin{align}
    \sum_\rho \ket{e_\rho}\bra{e_\rho} \neq 1 \text{(unless the basis is
        orthonormal)}
    % TODO think about this in depth
\end{align}
Therefore we need anthoer argument for $\Omega$ being positive-definite.
It is provided in page 11 of \cite{book}.

\paragraph{Orthogonal matrix} For an orthogonal matrix over $\mathbb{C}$,
it is quite troublesome. For example, if $Ra=\lambda a$ and
$\lambda \neq \pm 1$, then we have $a^T a=0$, which is quite bad because
this force $a$ to have complex components.

\paragraph{Orthogonal matrix over $\mathbb{R}$} In this case, we have
similar result. But it is easy to show that for an orthogonal matrix
$R$ having only real elements, then its eigenvalues $\lambda= \pm 1$.

Then he proceeds to direct product.
\paragraph{Direct product} and also the Kronecker Product of two
matrices. Properties (let $T=R\otimes S$):
\begin{enumerate}
    \item $\mathrm{dim}T = \mathrm{dim}R \times \mathrm{dim}S$
    \item $\mathrm{tr}(T)=\mathrm{tr}(R)\mathrm{tr}(S)$
    \item $\otimes$ commutes with the operation of inverse, transpose,
        and transpose conjugation.
    \item \begin{align}
            \frac{d}{d\alpha} (R(\alpha)\otimes S(\alpha)) =
            R'(\alpha)\otimes S(\alpha) + R(\alpha)\otimes S'(\alpha)
    \end{align}
    \item when the dimentions are the same:
        \begin{enumerate}
            \item 
            $(R_1\otimes S_1)(R_2\otimes S_2) = (R_1R_2)\otimes (S_1S_2)$
        \end{enumerate}
\end{enumerate}

Finally we arrived in the group theory.
\paragraph{Symmetry examples} Dipole transition. 
$\braket{\phi_f|\hat{P}|\phi_i}$, must happen when the parity of $\phi_i$
and $\phi_f$ is of opposite parity. (pp.18 of \cite{book})

\paragraph{Group}
\begin{defi}[Group]
\nomenclature{Group}{\nomrefpage.}
    Omitted.
\end{defi}
Some basic properties (Omitted).
\begin{defi}[Abel Group]
\nomenclature{Abel Group}{\nomrefpage.}
    Omitted.
\end{defi}
\begin{defi}[Cardinality of group $\# A$]
\nomenclature{Cardinality of group $#$}{\nomrefpage.}
    Omitted.
\end{defi}

\paragraph{Multiplication table}

Facts: group of order $1,2,$ and $3$ are unique up to an isomorphism.

\begin{defi}[Cyclic group, generators]
\nomenclature{Cyclic group, generators}{\nomrefpage.}
    Omitted.
\end{defi}

\begin{CJK}{UTF8}{gbsn}
固有转动是指的那些 $det(M)>0$ 的转动. 用$C_n$来表示他们.

Also, 周期 of $R$ is just $\braket{R}$.
\end{CJK}

Let $\sigma$\nomenclature{$\sigma$}{\nomrefpage.} for spatial reflection.
\begin{defi}[$C_N,\bar{C}_N$]
\nomenclature{$C_N,\bar{C}_N$}{\nomrefpage.}
    $\bar{C}_N=C_N*\sigma$
\end{defi}
\section{20161010}
\label{sec:20161010}
Introducing to various groups:$S_4, V_4$, $D_3$, all omited.
(\textbf{pp.22-23 of \cite{book}})

$D_n$ group. See pp. 25-26 of the book \cite{book}.
Note that here the $n$ refers to the $n$-polygon, not that the group is
of order $n$. For the mathematicians, they might be comfortable with
$D_n$ means the dihedral group of order $n$, but is actually the group of
symmetries of $n/2$-polygon.

\begin{defi}[Subgroup]
\nomenclature{Subgroup}{\nomrefpage.}
    Omitted.
\end{defi}
\begin{fact}
    One only has to check the closeness for determining a subgroup, if it
    is of finite order.

    However, for group of infinite order, one has to check the existance
    of unit and inverse elements.
\end{fact}
Examples of subgroup (\textbf{pp.26 of \cite{book}})

Noteworth:$C_6$ has three copies of $D_2$, this can be intuitively 
guessed by the fact that a hexago has three rectanle in it.

\begin{defi}[Coset]
\nomenclature{Coset}{\nomrefpage.}
    Omitted.
\end{defi}
Properties of coset (omitted).

\begin{defi}[Index of subgroup]
\nomenclature{Index of subgroup}{\nomrefpage.}
    Omitted.
\end{defi}

\begin{prop}
    Two elements $R$ and $T$ belongs to the same coset $kH$, if and only
    if $R^{-1}T\in H$.
\end{prop}
\begin{proof}
    Omitted.
\end{proof}
\begin{defi}[Normal/Invariant subgroup]
\nomenclature{Normal/Invariant subgroup}{\nomrefpage.}
    A subgroupNormal/Invariant subgrouproup(also invariant), if and only
    if for any $x\in G$, we have $xH = Hx$.
\end{defi}
\begin{fact}
    If $H$ has index 2, then it must be normal/invariant. This is
    obvious.
\end{fact}
\begin{defi}[Quotient]
\nomenclature{Quotient}{\nomrefpage.}
    Omitted.
\end{defi}
Note that quotient group (a.k.a. factor group) is only defined for a normal subgroup.

\begin{ex}
    $D_3$ (Using the multiplication table). Omitted because it is too
    complex to be typed down here.
\end{ex}

\begin{defi}[Conjugate]
\nomenclature{Conjugate}{\nomrefpage.}
    If exists $S\in G$, s.t. $R' = S^{-1}RS$, then we say $R'$ is
    conjugate to $R$.
\end{defi}
see (\textbf{pp.28-30 of \cite{book}})
This is clearly an equivalence relationship. By this we can define
conjugate class, denoted by $C=\{R_1,\cdots\}$, then we have the
characterization $C = \{ s^{-1} R_i s|\forall s\in G\}$, for any
$R_i$. We then have the following facts (all are obvious):
\begin{fact}$ $

    \begin{enumerate}
        \item The unit class formed just by the unit element.
        \item The inverse class formed by just all the inverse element.
            $C^{-1} = \{ R_i^{-1}\}$. If $C=C^{-1}$, then $C$ is called
            self-inverse.
        \item The order of elements in a class is just the same.
        \item For $\forall T,S\in G$, $TS$ and $ST$ are conjugate to each
            other. This means that all elements symmetric on the
            multiplication table is conjugate to each other.
        \item For two elements $R$, $R'$ conjugate to each other, both
            can be expressed by the products of two elements in two
            different way. (Isn't this too obvious to mention.)
        \item Let $G$ be a rotation group. Suppose it has an axis of the
            order of $n$, with its operation denoted as $R$,, we can get
            a new axis by the following steps. (Supose we have another
            rotation $S$),

            \begin{enumerate}
                \item  $S^{-1}$, rotate $m$ back to $n$.
                \item  $R$, rotate about n around $2\pi/n$,
                \item  $S$ rotate $n$ to $m$
            \end{enumerate}

            Result: $S^{-1}RS$ rotate around a new axis $m$ about
            $2\pi/n$. So $R'=S^{-1}RS$ and $R$ is calle the equivalent
            axis. 
            
            Also, if $m=-n$, then they are called polar axis to each
            other.

        \item $C_n$, which is an abel group, every element form a
            conjugate class by itself. Specifically, $e$ and $R^{n/2}$
            are self inverse, if $n$ is even.
    \end{enumerate}
\end{fact}
\begin{prop}
    \label{prop:20161010.conjugate_subgroup}
    For an invariant subgroup, then every conjugate element is also
    inside the same invariant subgroup. This shows that an invariant
    subgroup can be decomposed into a series of sum of conjugate classes.
\end{prop}
\begin{proof}
    If $R\in H$, then we show that $S^{-1}RS\in H$, this is obviously
    since it belongs to $S^{-1}HS$.
\end{proof}

\begin{ex}
    For $D_3: E,D,F,A,B,C$, their orders are respetively
    $1,3,3,2,2,2$. We have the following conjugate classes:
    \begin{enumerate}
        \item $\{ E\}$. Is self inverse.
        \item $\{ D,F\}$. $D$ is conjugate to $F$. We can see
            this physically by looking at rotation from the front
            or the below. This class is also self-inverse.
        \item $\{A,B,C\}$, is clearly a self-inverse conjugate
            class.
    \end{enumerate}
\end{ex}
\begin{ex}[$D_6$]
    Ramiliarize one with the formulae for $D_n$. Hint: use the order of
    elements to find classes of conjugate. Then use the proposition
    \ref{prop:20161010.conjugate_subgroup} to find the subgroups.
    % TODO maybe I can try this when I am free.
\end{ex}





\section{Skipped Two lectures}
Due to GRE physics preparation. Concepts that may have been covered:

\begin{itemize}
    \item Representation of Groups.
        \begin{itemize}
            \item Character of representation.
            \item Equivalence between representation.
        \end{itemize}
    \item Transformation of Fields
    \item Regular Representation.
\end{itemize}

Now let's me cover these concepts quickly.

\begin{defi}[Representation]
\nomenclature{Representation}{\nomrefpage.}
    A representation of a group $G$ is a continuous homomorphism $D$
    from $G$ to the group of automorphisms of a vector space $V$:
    \begin{equation}
        D: G\mapsto \mathrm{Aut}V
    \end{equation}
    $V$ is called the \textit{represetation space}, and the
    \textit{dimension of the representation} is the dimension of $V$.
\end{defi}

The following is copied from \cite{Ludeling}.

\begin{itemize}
    \item There is always the representation $D(g) = 1$ for all $g$.
        If $\mathrm{dim} V = 1$, this is called the \textit{trivial
        representation} \nomenclature{trivial
        representation}{\nomrefpage}.
    \item The matrix groups, i.e. $\mathrm{GL}(n, K)$ and subgroups,
        naturally have the representation "by themselves", i.e.
        by$n\times n$ matrices acting on $K_n$ and satisfying the
        defining constraints (e.g. nonzero determinant). This is
        loosely called the \textit{fundamental or defining
        representation}.
    \item Two representations $D$ and $D'$ are called
        \textit{equivalent}\nomenclature{equivalent of
        representation}{\nomrefpage} if they are related by a
        similarity transformation, i.e. if there is an operator S such
        that
        \begin{equation}
            SD(g)S^{-1} = D'(g)
        \end{equation}
        for all $g$. Note that $S$ does not depend on $g$! Two equivalent
        representation can be thought of as the same representation in
        different bases. We will normally regard equivalent
        representations as being equal.

        Note also here $S$ is the transformation of basis. If we know
        the transformation of vectors $X$, then
        \begin{equation}
            X^{-1} D(g)X = D'(g)
        \end{equation}
    \item A representation is called
        \textit{faithful}\nomenclature{faithful
        representation}{\nomrefpage}if it is injective, i.e.
        $\mathrm{ker} D = \{e\}$, or in other words, if $D(g_1) \neq
        D(g_2)$ whenever $g_1 \neq g_2$.
    \item If $V$ is equipped with a (positive definite) scalar
        product, D is unitary if it preserves that scalar product, i.e
        if
        \begin{equation}
            \braket{u,v} = \braket{D(g)u,D(g)v}
        \end{equation}
        for all $g\in G$. (Here we assume that $V$ is a complex vector
        space, as that is the most relevant case. Otherwise one could
        define orthogonal representations etc.)
\end{itemize}

\section{20161031}
\label{sec:20161031}

\begin{thm}[Unitary Representation]
    For finite groups and for compact Lie groups, all representations
    are equivalent to a unitary representation.
\end{thm}
\begin{proof}
    For $D(g)$, we need to find a $X$ such that $\bar{D}(g)\equiv
    X^{-1}D(g)X$ is unitary.

    Since
    \begin{align*}
        1 = \bar{D}^\dagger \bar{D}
    \end{align*}
    One will find 
    $$ (XX^\dagger)^{-1} = D^\dagger (XX^\dagger)^{-1} D$$
    Then let
    \begin{equation}
        H\equiv \sum_{s\in G} D^\dagger(s) D(s)
    \end{equation}
    One can verify that
    \begin{equation}
        D^\dagger (g) H D(g) = H
    \end{equation}

    Now we construct $X$ from $H$. We have
    \begin{equation}
        H = (XX^\dagger)^{-1}
    \end{equation}

    Notice we have $H$ is Hermitian by above equation. Also, $H$ is
    positive definite (easily seen from the definition of $H$ and 
    $a^\dagger H a \geq 0$. Also $H$ is of full rank.).

    Then we have $UHU^{-1} = \mathrm{diag}\{\gamma_1,\gamma_2,\cdots\}$
    and $\gamma_i > 0$. The rest for constructing $X$ should be
    obvious.

    Another way to prove this is to imbe
\end{proof}

\begin{thm}
    For any two equivalent representation, there is always a unitary
    matrix to relate the two, i.e. exits $Y$ unitary, s.t.
    $$ \bar{D}(g) = Y^{-1} D(g) Y$$
\end{thm}
\begin{proof}
    Suppose we have two unitary representation $D(g)$ and
    $\bar{D}(g)$, related by
    $$ \bar{D}(g) = X^{-1} D(g) X$$
    where $X$ is not necessarily unitary.

    Let $H\equiv X^\dagger X$, it is easy to prove that $H$ is
    Hermitian and positive definite. Direct calculation shows that
    $$ \bar{D}^{-1}(g) H \bar{D}(g) = H$$
    That is $\bar{D}(g)$ and $H$ commute.
    Then we construct $Y$ from $H$. Let $V$ be such that
    \begin{equation}
        V^{-1} H V = \Gamma
    \end{equation}
    where $\Gamma$ is a diagonal matrix of $H$'s eigenvalues.
    Obviously $V$ has to be unitary. Define $Y$ to be
    \begin{equation}
        Y\equiv V \sqrt{\Gamma} V^{-1}
    \end{equation}
    By direct calculation, we have
    $$ (XY)^\dagger (XY) = 1$$
    One can show that
    $$[ Y, \bar{D}(g)] = 0$$
    with laborious calculation. % TODO 
\end{proof}

\begin{defi}[Reducibility of Representation]
    \nomenclature{Reducibility of Representation}{\nomrefpage.}
    A representation $D$ is called \textit{reducible} if $V$ contains an
    invariant subspace. Otherwise $D$ is called irreducible.

    A representation is called \textit{fully reducible} if $V$ can be
    written as the direct sum of irreducible invariant subspaces, i.e.
    $V=V_1\oplus \cdots \oplus V_p$, all the $V_i$ are invariant and the
    restriction of $D$ to each $V_i$ is irreducible.
\end{defi}

\begin{ex}$ $

    \begin{itemize}
        \item The representation of finite group is obviously fully
            reducible or irreducible.
        \item The representation of translation group is not fully
            reducible. For example, for translation we have:
            $T_a T_b = T_{a+b}$,
            One can confirm that the following representation obeys
            the above relationship:
            $$T_a = \left( \begin{array}{cc}
                 1 & a \\
                 0 & 1 \\
            \end{array} \right)$$
            but this is obviously not fully reducible.
    \end{itemize}
\end{ex}

\begin{defi}[Interwiner]
\nomenclature{Interwiner}{\nomrefpage.}
    Given two representations $D_1$ and $D_2$ acting on $V_1$ and
    $V_2$, an intertwiner between $D_1$ and $D_2$ is a linear operator
    \begin{equation}
        F: V_1 \mapsto V_2
    \end{equation}
    which "commutes" with $G$ in the sense that
    \begin{equation}
        F D_1(g) = D_2(g) F
    \end{equation}
    for all $g\in G$.
\end{defi}

(From pp.30 of \cite{Ludeling})

The existence of an intertwiner has a number of consequences. First,
$D_1$ and $D_2$ are equivalent exactly if there exists an invertible
intertwiner. Second, \hl{the kernel and the image of $F$ are invariant
subspaces}: Assume $v\in \Ker{F}$, i.e. $Fv = 0$. Then
\begin{equation}
    FD_1 v = D_2 F v = D_2 0 = 0
\end{equation}
so $D_1 v\in \Ker{F}$. On the other hand, let $w_2 = Fw_1$ be an
arbitrary element of the image of $F$. Then from the definition we have
\begin{equation}
    D_2 w_2 = D_2 F w_1 = FD_1 w_1
\end{equation}
which is again in the image of $F$. Now if $D_1$ is irreducible, the only invariant subspaces,
hence the only possible kernels, are $\{0\}$ and $V_1$ itself, so $F$ is either injective or zero.
Similarly, if $D_2$ is irreducible, F is either surjective or zero. Taking these statements together, we arrive at Schur’s Lemma: 

\begin{lemma}[Schur's lemma 1]
    An intertwiner between two irreducible representations is either
    an isomorphism, in which case the representations are equivalent,
    or zero
\end{lemma}

An important special case is the one where $D_1 = D_2$. In that case,
we see that $F$ is essentially unique. More precisely, we have the
following theorem, also often called Schur’s Lemma:
\begin{lemma}[Schur's lemma 2]
    If $D$ is an irreducible finite-dimensional representation on a complex
vector space and there is an endomorphism $F$ of $V$ which satisfies 
\begin{equation}
    FD(g) = D(g) F
\end{equation}
for all $g\in G$, then F is a multiple of the identity, $F = \lambda
\id$
\end{lemma}
\begin{proof}
    Note that $F$ has at least one eigenvector $v$ with eigenvalue
    $\lambda$. (This is where we need $V$ to be a complex vector
    space: A real matrix might have complex eigenvalues, and hence no
    real eigenvectors.) Clearly, $F-\lambda\id$ is also an
    intertwiner, and it is not an isomorphism since it annihilates
    $v$. Hence, by Schur’s Lemma, it vanishes, thus $F = \lambda\id$.
\end{proof}

(From the book \cite{book})
\begin{thm}[Orthogonal Theorem]
    For finite group $G$, let $D^i(G)$ and $D^j(G)$ be its two
    irreducible, non-equivalent, and unitary representation. Then, as
    a vector in group algebra, they have the following orthogonal
    relationship:
    \begin{equation}
        \sum_{R\in G} D^{i*}_{\mu\rho}(R)D^j_{\nu \lambda}(R) =
        \frac{g}{m_j} \delta_{ij} \delta_{\mu\nu}\delta_{\rho\lambda}
    \end{equation}
    $g$ is the order of the group, and $m_j$ is the
    dimension of representation $D^j(G)$.
\end{thm}
\begin{proof}
    Let
    \begin{equation}
        Y_{\rho\lambda}^{\mu\nu} \equiv
        \delta_{\rho\lambda}\delta_{\mu\nu}
    \end{equation}
    Then let
    \begin{align*}
        X^{\mu\nu} = \sum_{R\in G} D^i(R^{-1}) Y^{\mu\nu} D^j(R)
    \end{align*}
    One can find by direct calculation
    \begin{align}
        X^{\mu\nu}_{\rho\lambda}= \sum_{R\in G}
        D^{i*}_{\mu\rho}(R)D^j_{\nu \lambda}(R)
    \end{align}
    And also through direct calculation, one finds
    \begin{align*}
        D^i (s) X^{\mu\nu} = X^{\mu\nu} D^j(s)
    \end{align*}
    for any $s\in G$. 
    Then $X^{\mu\nu}$ is a interwiner. So the case for $i\neq j$ is
    obvious. When $i=j$, we have:
    $$ X = \lambda \id$$
    Now we find the $\lambda$, i.e. the eigenvalue of $X^{\mu\nu}$.
    
    Now since
    \begin{align*}
        X^{\mu\nu}_{\rho\lambda} = \lambda^{\mu\nu}
        \delta_{\rho\lambda}
    \end{align*}
    One can find two fact by direct calculation:
    \begin{align*}
        &\sum_\rho X^{\mu\nu}_{\rho\lambda} = m_j \lambda^{\mu\nu} \\
        &\sum_\rho X^{\mu\nu}_{\rho\lambda} = g \delta^{\mu\nu}
    \end{align*}
    Hence $\lambda^{\mu\nu} = \frac{g}{m_j} \delta^{\mu\nu}$.
\end{proof}
\begin{remark}
    For the first orthogonality $\delta_{ij}$, one actually do not
    required that $D^i$ and $D^j$ is unitary.
\end{remark}
\begin{coro}
    \begin{equation}
        \sum_{j=1}^{l} m_j^2 \leq g
    \end{equation}
    % TODO
\end{coro}
\begin{coro}
    \begin{equation}
        \sum_{R\in G} \chi^{i*}(R)\chi^j(R) = g \delta_{ij}
    \end{equation}
\end{coro}
\section{Anchor}
\label{sec:Anchor}
\begin{thebibliography}{1}
    \bibitem{book} Zhongqi Ma, Group Theory in Physics
    \bibitem{math.se_1_lenz_vector} \href{physics.stackexchange.com/questions/18088/what-symmetry-causes-the-runge-lenz-vector-to-be-conserved}{What symmetry causes the Runge-Lenz vector to be conserved?}
    \bibitem{Ludeling} Lecture Notes for physics751: Group Theory (for
    Physicists), by C Ludeling.
    \href{http://www.th.physik.uni-bonn.de/nilles/people/luedeling/grouptheory/data/grouptheorynotes.pdf}{Link}
\end{thebibliography}
\printnomenclature
\section{License}
The entire content of this work (including the source code
for TeX files and the generated PDF documents) by 
Hongxiang Chen (nicknamed we.taper, or just Taper) is
licensed under a 
\href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative 
Commons Attribution-NonCommercial-ShareAlike 4.0 International 
License}. Permissions beyond the scope of this 
license may be available at \url{mailto:we.taper[at]gmail[dot]com}.
\end{document}
