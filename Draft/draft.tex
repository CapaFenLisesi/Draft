% The entire content of this work (including the source code
% for TeX files and the generated PDF documents) by 
% Hongxiang Chen (nicknamed we.taper, or just Taper) is
% licensed under a 
% Creative Commons Attribution-NonCommercial-ShareAlike 4.0 
% International License (Link to the complete license text:
% http://creativecommons.org/licenses/by-nc-sa/4.0/).
\documentclass{article}

\usepackage{float}  % For H in figures
\usepackage{amsmath, amssymb} % For math
\usepackage{mathtools} % dcases*, see https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics#The_cases_environment
\numberwithin{equation}{subsection} % have the enumeration go to the subsection level.
									% See:https://en.wikibooks.org/wiki/LaTeX/Advanced_Mathematics
\usepackage{graphicx}   % need for figures
\usepackage{cite} % For bibligraphy
\usepackage{fancyref} % For lazy reference \fref
\usepackage[unicode]{hyperref} % For hyperlink everything.
\usepackage{CJKutf8} % For Chinese characters
%\usepackage{ dsfont } % For double struck fonts
\usepackage{braket} 
\usepackage[T1]{fontenc}
\usepackage{listings}

\usepackage{amsthm}
\newtheorem{defi}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{coro}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{ex}{Example}[section]

\usepackage{tensor}  % For tensor indices
\usepackage[all]{xy} % For drawing category diagrams
\usepackage{mathrsfs}

\title{Draft}
\date{\today}
\author{we.taper}
\begin{document}
\maketitle
\abstract{
    This is a draft.
}
\tableofcontents

\section{Problems in Bernevig's Topological ...}

    \subsection{Chapter 2}
    \paragraph{Non-Abelian Berry Transport}
    Derive Berry curvature to the adiabatic transport of a degenerate
    multiplet of states separated by a gap from the excited states.
    (Cautious about rotation within degenerate states).

    Answer: $\gamma_{mn}(t) = i \int_0^t \braket{m(R(t'))|\frac{d}{dt'}|n(R(t'))}dt'$

    Approach:
    Assuming that the those degenerate states are labeled by $1\cdots N$. 
    Thus we have naturally:
	\begin{align}
            H\phi &=i\hbar\frac{\partial}{\partial t}\phi\text{,  }
        \phi = \sum_n A_n \psi_n
    \end{align}
    
    Then we have:
    \begin{align}
    H \sum_n A_n \psi_n &=i\hbar\frac{\partial }{\partial t}
	  	\sum_n A_n(t) \psi_n(R(t)) \nonumber\\
	\sum_n E A_n \psi_n&=i\hbar\sum_n
		\left(
		\frac{\partial A_n(t)}{\partial t}\psi_n(R(t))+
		A_n(t)\frac{\partial \psi_n(R(t))}{\partial t}
		\right) \nonumber\\
	E A_m &=i\hbar
		\frac{\partial A_m(t)}{\partial t}
		+\sum_n A_n(t)\braket{m|\frac{\partial }{\partial t}|n}
    \end{align}
    Put in another form:
    $$
    \sum_n \left(\delta^n_m E
        - \braket{m| \frac{\partial}{\partial t}|n} \right) A_n
        =
    i\hbar \frac{\partial A_m(t)}{\partial t}
    $$
    In matrix form:
    \begin{align}
            (E - P) \mathbf{A} = i\hbar \dot{\mathbf{A}}
    \end{align}
    where:
    \begin{align}
        E &= \left(
        \begin{array}{ccc}
         \cdots   &     &   \\
              & E &   \\
             &    & \text{...} \\
        \end{array}
        \right) \\
        P &= (P^m_n) = \left(
        \braket{m| \frac{\partial}{\partial t}|n}\right)\\
        A &= \left(\begin{array}{c}
            A_1(t) \\
            A_2(t) \\
            \cdots \\
        \end{array}\right)
    \end{align}
    
    Note that $\braket{n| \frac{\partial}{\partial t}|m}^* \neq
    \braket{m| \frac{\partial}{\partial t} |n}$, thus $P$ may not be
    Hermitian.Ergo $E-P$ is Hermitian. So it is diagonalizable.

    Notice that
    \begin{align}
            0= \frac{\partial}{\partial t} \braket{m|n} &=
        \braket{\frac{\partial}{\partial t}m|n} +
        \braket{m|\frac{\partial}{\partial t}n} &&(\text{any }m,n)
    \end{align}
    temporary mathematica code:

% In[8]:=DSolve[{y1'[x]+y2'[x]==x,y1'[x]-y2'[x]==0},
%         {y1[x], y2[x]}, x]
% In[9]:= DSolve[{e1*A1[t]==A1'[t]+A1[t]*m1+A2[t]*m2, 
% e1*A2[t]==A2'[t]+A1[t]*m1+A2[t]*m2},{A1[t],A2[t]},t]
% In[10]:=DSolve[{e1*A1[t]==A1'[t]+A1[t]*m1},{A1[t]},t]
    

\section{Quantum Field Theory}
\label{sec:Quantum_Field_Theory}
    \subsection{Relatvistic Quantum Mechanics}
    \label{sec:Relatvistic_Quantum_Mechanics}

    \subsubsection{2.1 Quantum Mechanics}
    \label{sec:2.1_Quantum_Mechanics}
    This part sumarize the axioms of quantum mechanics. (Omitted)
    However, one class of important but seldom mentioned operator,
    the antilinear operators and the antiunitary operators are not
    discussed here but postponed to the next part.

    \subsubsection{2.2 Symmetries}
    \label{sec:2.2_Symmetries}
    This part discribe some important theorems concerning the symmetries
    in quantum mechanics.

    Firstly, symmetries in quantum mechanics some times requires the use
    of antilinear and antiunitary operators.

    \paragraph{Antilinear and Antiunitary}
    Antilinear operators: $U (\lambda A + \upsilon B)
                            = \lambda^* UA + \upsilon^* UB$.

    Antiunitary: $ (U \phi, U \psi) = (\psi, \phi) = (\phi,\psi)^*$

    The adjoint operator of an antilinear operator requires special
    attention:
    $$ (\phi, A^\dagger\psi) = (A\phi, \psi)^*$$
    (The usual definition will become troublesome since the LHS will
    be antilinear in $\phi$, whereas the RHS is linear.)

    Thankfully, the criterion for unitarity or antiunitarity is the same:
    $$U^\dagger = U^{-1}$$
    
    An important example is those symmetries which can be infinitesimally
    close to identity. In the vincinity of unity, it is:
    $$ U =1+i\epsilon t $$
    where $t$ is Hermitian and linear.
    % TODO why is it Hermitian?

    \paragraph{Symmetry Group and its Representation} The set of all
    symmetries transformations obviously can have a group structure.
    By giving each symmetry transformation a unitary or antiunitary
    operator, a representation of such group is obtained. However,
    such a representation can be projective (projective as in
    projective geometry, or $\mathbb{CP}^n$), since operators acts
    on ket spaces, which is already projective (within a freedom of
    phase). 

    If the phase is taken into consideration, it is found that this
    phase is independent of the ket that the operator acts on:
    \begin{align}
        \label{eq:2.2_symmetries_projective_phase}
        U(T_1 T_2) \Psi = e^{i\phi} U(T_1) U(T_2) \Psi
    \end{align}
    Here $U(T_1)$/$U(T_2)$ is the operator corresponding to the symmetry
    transformation $T_1$/$T_2$. $\phi$ is the phase, depends only
    on $T_1 T_2$.

    Howver, there is one exception to this rule. It exists when the
    state $\Psi$ is not the linear some of some other wave functions.
    That is, we can not express $\Psi$ as $\sum_i \lambda_i \psi_i$.
    This seems incomprehesible to me.
    % TODO Understand this!
    There are some link on page 53
    between the structure of Lie group associated
    with this symmetry and whether the representation can be projective.
    \footnote{Though not state, it can be infered that a representation
    can be made not-projective by a good choice of $U(T)$, so that
    $\phi \equiv 0$ in \fref{eq:2.2_symmetries_projective_phase} }

    \paragraph{Connected Lie group} is of special importance in physics.
    However, the books description obfuscate the mathematical description
    of this structure. I will update this note later to remedy such
    % TODO update this part.
    discussion. The message I understand is that the representation is
    tightly bound to the Lie algebra. An important example is that when
    $$ U(T(\theta)) \approx 1 + i \theta^a t_a $$
    $$ [t_b,t_c] = 0$$
    then
    $$ U(T(\theta)) = \exp(it_a \theta^a) $$

    \subsubsection{2.3 Quantum Lorentz Transformations}
    \label{sec:2.3_Quantum_Lorentz_Transformations}
    \textbf{Note}: although the title suggests "quantum", this part is
    mostly classical.

    Starting with the invariance of interval:
    \begin{align}
        \label{eq:2.3_Quantum_Lorentz_Transformations invariance_of_interval}
        \eta_{\mu\nu}dx'^\mu dx'^\nu = \eta_{\mu\nu}dx^\mu dx^\nu
    \end{align}
    or equivalently % TODO why equivalently?
    \begin{align}
        \label{eq:2.3_Quantum_Lorentz_Transformations invariance_of_interval 2}
        \eta_{\mu\nu} \frac{\partial x'^\mu}{\partial x^\rho}
            \frac{\partial x'^\nu}{\partial x^\sigma}
            =
        \eta_{\rho\sigma}
    \end{align}
    It claims that any coordinate transformation satisfying 
    \ref{eq:2.3_Quantum_Lorentz_Transformations invariance_of_interval 2}
    must be linear.

    Such linear Lorentz transformations satisfy:
    \begin{align}
        T(\bar{\Lambda},\bar{a})T(\Lambda,a) = 
            T(\bar{\Lambda}\Lambda, \bar{\Lambda}a+\bar{a})
    \end{align}
    where $\bar{\Lambda}+\bar{a}$ and $\Lambda+a$ are two such 
    transformations.

    The $\Lambda$ and $\Lambda^{-1}$ can be determined by several 
    equations that relate it with $\eta_{\mu\nu}$. 
    
    Note that:
    \[ \Lambda\indices{^\rho_\nu} \neq \Lambda\indices{_\nu^\rho} \]
    And:
    \begin{equation}
        \label{eq:2.3_Quantum_Lorentz_Transformations transformation_matrix}
        (\Lambda^{-1})\indices{^\rho_\nu} = 
            \Lambda\indices{_\nu^\rho} = 
            \eta_{\nu\mu}\eta^{\rho\sigma} \Lambda\indices{^\mu_\sigma}
    \end{equation}

    The only part about quantum mechanics here, is that the operators
    corresponding to the Lorentz transformations, have the property that:
    \begin{equation}
        U(\bar\Lambda,\bar{a})U(\Lambda, a) = 
            U(\bar\Lambda\Lambda, \bar\Lambda a + \bar{a})
    \end{equation}
    where $a$ represents the translation.
    
    The group of Lorentz transformations is very important. There is:
        %\text{sssssss} & B
    \[
        \xymatrix{ 
        \text{Poincar\'e Group}  \\
        \text{Homogeneous Lorentz group} \ar@{^{(}->}[u] \\
        \text{proper orthochronous Lorentz group}\ar@{^{(}->}[u]
        }
    \]
    The important \textit{proper orthochronous Lorentz group} consists
    of those $\Lambda$ with $\det \Lambda = 1$ and 
    $\Lambda\indices{^0_0} \geq 1$.
    
    We have also $ \mathscr{P}$, the space inversion matrix. And
    \(\mathscr{T}\), the time-reversal matrix. The definition for
    both can be easily written.

    A important rule is that all homogeneous Lorentz transformations can
    are generated by 
    \(\{\text{proper orthochronous}, \mathscr{P}, \mathscr{T} \}\).
    However this property is not proved

    \paragraph{Digression about tensor notation} It seems that phsysicists
    have not settled down on their notation for tensors. Weinberg using
    the notation $\Lambda\indices{^\nu_\mu} \equiv \Lambda(e^\nu,e_\mu)$,
    so it acts on $(v,w)$, where $v$ is a cotangent vector and $w$ is a
    tangent vector. I see that this notation helps to distinguish the
    type of arguments. This is unnecessary, since the upper and lower
    indices already fulfill this function. Another benefit is that it
    conveies the process of lowering of raising a index. 
    
    However, this idea that each slot in tensor indices are distinct
    and can be lowered and raised seperatly, is not even mentioned
    in some modern mathematical physics book.

    Helpful link:
    \href{http://physics.stackexchange.com/questions/158309/convention-of-tensor-indices}{Convention of tensor indices in Phy.SE}, and 
    \href{http://physics.stackexchange.com/questions/237270/working-with-indices-of-tensors-in-special-relativity?noredirect=1&lq=1}{Working with indices of tensors in special relativity in Phy.SE}.

    \subsubsection{2.4 The Poincar\'{e} Algebra}
    \label{sec:2.4_The_Poincare_Algebra}
    \textbf{I am lost in this part.} The general idea seems to develop, in a 
    infinitesimal sense, the property of a Lorentz transformation.
    For a infinitesimal Lorentz transformation:
    \begin{align}
        U(1+\omega, \epsilon) = 
            1+ \frac{1}{2} i\omega_{\rho\sigma}J^{\rho\sigma}
            -i\epsilon_\rho P^\rho + \cdots
    \end{align}
    where $J$ and $P$ are independent of the infinitesimal value
    $\omega$ and $\epsilon$.

    Later $P$ is identified as the energy-momentum operator (with
    $P^0$ being the energy operator), and $J^{23}$, $J^{31}$,
    \(J^{12}\) are identified with the angular momentum operator.
    However, the reason for this identification is not provided. 
    % TODO why P is energy momentum, and J is angular momentum

    Later, it was eatablished that $J$ and $P$ satisfy:
    \begin{align}
        i[J^{\mu\nu},J^{\rho\sigma}] &=
            \eta^{\nu\rho}J^{\mu\sigma} - \eta^{\mu\rho}J^{\nu\sigma}
            -\eta^{\sigma\mu}J^{\rho\nu} + \eta^{\sigma\nu}J^{\rho\mu}
        \\
        i[P^\mu,J^{\rho\sigma} &=
            \eta^{\mu\rho}P^\sigma - \eta^{\mu\sigma}P^{\rho}
        \\
        [P^\mu,P^\rho] &= 0
    \end{align}
    This is called the \textit{Lie Algebra} of the Poincar\'{e}
    group.
    The relation \([P^\mu,P^\rho] = 0\) is particular interesting.

    With this, the finite translation and a rotation by angle
    $\theta$ are expressed as:
    \begin{align}
        U(1,a) = \exp(-iP^\mu a_\mu) \\
        U(\theta,0) = \exp(i \mathbf{J}\cdot \theta)
    \end{align}

    This part ends with a discussion on the low-velocity limit
    of the Lie Algebra obtained above, i.e. the Galilean algebra.
    
    \subsection{Note}
    \label{sec:Note}
    I will now change to another book: Q.F.T. in a Nutshell by A. Zee.


\section{Miscellnaneous Math}
\label{sec:Miscellnaneous_Math}
\subsection{\texorpdfstring{$\int_0^\infty \frac{\sin(x)}{x}$}{}}
From \href{http://math.stackexchange.com/questions/5248/solving-the-integral-int-0-infty-frac-sinxx-dx-frac-pi2}{Math.SE}. 
By \href{http://math.stackexchange.com/users/1102/aryabhata}{Aryabhata}.

I believe this can also be solved using double integrals.

It is possible (if I remember correctly) to justify switching the order of integration to give the equality:

$$\int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dy \Bigg)\, dx = \int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dx \Bigg)\,dy$$
Notice that
$$\int_{0}^{\infty} e^{-xy} \sin x\,dy = \frac{\sin x}{x}$$

This leads us to

$$\int_{0}^{\infty} \Big(\frac{\sin x}{x} \Big) \,dx = \int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dx \Bigg)\,dy$$
Now the right hand side can be found easily, using integration by parts.

\begin{align*}
I &= \int e^{-xy} \sin x \,dx = -e^{-xy}{\cos x} - y \int e^{-xy} \cos x \, dx\\
&= -e^{-xy}{\cos x} - y \Big(e^{-xy}\sin x + y \int e^{-xy} \sin x \,dx \Big)\\
&= \frac{-ye^{-xy}\sin x - e^{-xy}\cos x}{1+y^2}.
\end{align*}
Thus $$\int_{0}^{\infty} e^{-xy} \sin x \,dx = \frac{1}{1+y^2}$$
Thus $$\int_{0}^{\infty} \Big(\frac{\sin x}{x} \Big) \,dx = \int_{0}^{\infty}\frac{1}{1+y^2}\,dy = \frac{\pi}{2}.$$


\section{Miscellnaneous Notes}
    \subsection{Super conductor}
    Mean-field approach to deal with a four operator diagonalization.
    
    Suppose we have: $D^*C^* CD$, then let $\delta = CD - \braket{CD} =
    CD - avg$. Then if we assume $\braket{CD}\neq 0$, and $\delta \approx 0$. Then we have:
    
    	$$ \delta^2 \approx 0 $$
    i.e.:
    
    \begin{align}
    	( (CD)^* - avg ) ( CD - avg ) = 0\\
    	D^*C^*CD = avg*(CD+D^*C^*) - avg^2
    \end{align}
    Hence a four operator is reduced into a few of two operators.
    Such method could be naturally extended to treat the operator
    $\sum_{i,j} D^*_i C^*_i C_j D_j$.
    
    A copper pair has the energy of:
    $$\Delta = \braket{C_{k \uparrow}C_{-k \downarrow} }$$
    
    To resist the flow of current carried by Copper Pair, is equivlant to destroying a pair of Copper Pair:
    $$ \braket{C_{k \uparrow}C_{-k \downarrow} } \longrightarrow C_{k \uparrow}C_{-k \downarrow}$$
    
    This will require an additional enegy of $2\Delta$.
    
    The exact meaning of "equivalent to" is as follows:
    \begin{align}
        & \text{break a copper pair} \longrightarrow 
        \text{scatter two electrons consecutively} 
        \nonumber\\ & \longrightarrow 
        \text{create two electron-hole mixed type quasi-particle} \longrightarrow 2\Delta \nonumber
    \end{align}

    \subsection{Why 0/0 is undefined?}
    If we suppose
    $$ \frac{0}{0}= \triangle $$
    Consider the following derivation:
    \begin{align}
	    \frac{0}{0} \cdot 1 &= \triangle \cdot 1 = \triangle \\
	    0 \cdot \frac{1}{0} &= \triangle\\
	    \Rightarrow \triangle &= 0
    \end{align}
    This is already bad enough. And we are forced to define $\frac{1}{0}$.
    Let $\frac{1}{0} = \square$, which literally means $1=0\cdot \square = 0$.
    This is disastrous.

    Alternatively, we could let
    \begin{enumerate}
	    \item Let $\frac{1}{0}$ be undefined.
	    \item Or let $\frac{1}{0} =
		    \infty$.
	    \item Or, let $\frac{a}{b}\cdot c= a\cdot \frac{c}{b}$ be not 
		    true when $b=0$.
    \end{enumerate}
    The third idea is disastrous for algebraic manipulation. \footnote{
    Or more speicifically, it is a disaster for field theory.
    }
    The first idea is not good. Since defining $\frac{1}{0}=\infty$ turns
    out to be very useful in both mathematics and physics. Actully, in
    physics it is common practice to set $\frac{a}{0}=\pm\infty$ for
    any nonzero number $a$, where the sign of $\infty$ is determined by 
    the sign of $a$.
    The second idea is okey. But then we are faced with a serious problem.
    We have to define $\triangle \equiv 0 \cdot \infty$

    $\triangle \cdot 2 = \triangle$, What will be of $\triangle + 1$?

    \subsection{Preface of BSCS}
        \label{sec:Preface_of_Bosonization_and_Strongly 
        Correlated_Systems}
    BSCS: see \cite{BSCS}.
    Parallism between theories in condensed matter physics and those in
    particle physics.
    \begin{itemize}
            \item Anderson-Higgs Phenomenon (Paritcle), Meissner effect
                    (C.M.P.)
            \item 'inflation' in Cosmology, first order phase transition
            \item 'cosmic strings', magnetic field vortex lines in type
                    II superconductors
            \item Hadron-meson interaction, Ginzburg-Landau theory of
                    superfluid $He^3$.
    \end{itemize}
    Same ideas on different space-time scales, different hierachical
    'layers'.
    Strong parallism: \textbf{strongly correlated low dimensional system}

    E.g.:

    The problem of formation and structure of heavy particles - hadrons and mesons. The corresponding fine structure constant $\alpha_G\approx 1$.

    Approaches:
    \begin{enumerate}
            \item Exact solutions
            \item Reformulate complicated interacting models in such a way
                    that they become weekly interacting. -> Bosonization.
                    
                    Spin $1/2$ anisotropic Hisenberg chain $\approx$
                    Model of interacting

                    fermions.
                    (Jordan and Wigner, 1928)
    \end{enumerate}
    Bosonization: transformation from fermions to a scalar massless bosonic
    field.


    \subsection{System of Differential Equations}
    This is a small note of \cite{DETA}.

    pp. 266.

    \begin{defi}
        $\mathbf{x(t)}$ is a vector whose elements are $x_i(t)$.
        $ \frac{d}{d t}$ acts on vector $\mathbf{x}$ element-wise.
        $\dot{\mathbf{x}}$ is abbrevation for $\frac{d}{d t}\mathbf{x}$
    \end{defi}
    
    pp. 291.

    \begin{thm}[Existence-uniqueness theorem]
        There exists one, and only one, solution of the initial-value
        problem

        \begin{align}
            \dot{\mathbf{x}}=\mathbf{A}\mathbf{x}\text{, }&
                \mathbf{x}(t_0) = \mathbf{x}^0 = 
                \left(
                \begin{array}{c}
		            x^0_1\\
                    x^0_2\\
                    \cdots
                \end{array} 
                    \right)
        \end{align}
        
        Moreover, this solution exists for $-\infty\langle t\langle \infty$.
    \end{thm}
    \begin{remark}
        By this, any non-trivial solution $\mathbf{x}(t)\neq 0$ at any
        time $t$. Also notice that the elements of $\mathbf{A}$ are just
        numbers.
    \end{remark}
    
    \begin{thm}
        The dimension of the space $\mathbf{V}$ of all solutions of the
        homogeneous linear system of differential equations:
        \begin{align}
            \frac{d\mathbf{x}}{dt}=\mathbf{Ax}
        \end{align}
        is $n$, i.e. the dimension of vector $\mathbf{x}$.
    \end{thm} 
    \subsection{ODE by Arnold}
    sec. 14
    \begin{defi}
        \begin{align}
            \label{eq:e^A}
            e^A &= I + A + \frac{A^2}{2!} + \frac{A^3}{3!}\\
            \text{or}& \nonumber \\
            e^A &= \lim_{n\to \infty}(I+\frac{A}{n})^n
        \end{align}
        where $I$ is the identity matrix.
    \end{defi}
    Equivalance of the two definition will be addressed in the Theorem on
    pp. 165.

    Important theorems:
    \begin{thm}[pp. 158]
        The series $e^A$ converges for any $A$ uniformly on each set
        $X=\{A:||A||\leq a\}$, $a\in \mathbb{R}$.
    \end{thm}
    \begin{thm}[pp. 160]
        $$e^{At} = H^t$$
        where $H^t$ is the translation operator which sends every polynomial
        $p(x)$ into $p(x+t)$.
    \end{thm}
    \begin{thm}[pp. 163]
        $$\frac{d}{dt} e^{tA} = Ae^{tA}$$
    \end{thm}
    \begin{thm}[Fundamental Theorem of the Theory of Linear Equations with
        Constant Coefficients]
        The solution of:
        \begin{align}
            \label{eq:fund_thm_of_linear_eqs_const_coef}
            \dot{\mathbf{x}} = A\mathbf{x}
        \end{align}
        with initial condition $\phi(0) = \mathbf{x}_0$ is
        \begin{align}
            \mathbf{\phi}(t) = e^{tA}\mathbf{x}_0
        \end{align}
    \end{thm}
    
    Practically solution to
    $$ \dot{\mathbf{x}} = A\mathbf{x}$$
    (pp. 173, Sec 17)
    (Assuming $A$ is diagonalizable.)
    \begin{itemize}
        \item Find the eigenvectors $\xi_1,\cdots ,\xi_n$ and eigenvalues
            $\lambda_1,\cdots ,\lambda_n$. Use them as basis.
        \item Expand the initial condition in the new basis.
            \begin{align}
                \mathbf{x}_0=\sum_{k=1}^{n} C_k\xi_k
            \end{align}
        \item Then $\phi(t) = \sum_{k=1}^n C_k e^{\lambda_k t}\xi_k$
    \end{itemize}
    \subsection{Appearance of Gauge Structure in Simple Dynamical Systems}
    
    \begin{align}
        0=(\eta_b,\dot{\eta_a}) = (\eta_b,\dot{U}_{ac}\psi) +
            (\eta_b,U_{ac}\dot{\psi}_c
    \end{align}
    \subsection{Quantum Statistical Mechanics}
    \label{sec:Quantum Statistical Mechanics}
    \begin{defi}[Time Evolution Operator]
        The time evolution operator $U(t,t_0)$ is defined such that
        \begin{align}
            \label{eq:def_time_evo_op}
            \ket{\Psi(t)} = U(t,t_0) \ket{\Psi(t_0)}
        \end{align}
    \end{defi}
    It satisfy the relationship:
    \begin{align}
        \label{eq:def_time_evo_op 2}
        i\hbar \partial_t U(t,t_0)= H U(t,t_0)
    \end{align}
    This is obvious when substituting $U(t,t_0)$ into the Schrodinger 
    Equations.
    
    \paragraph{Quantum Macrostates}
    Macrostates of the system depend on only a few the thermodynamic
    functions. We can form an ensemble of a large number $\mathcal{N}$ of
    microstates $\{\psi_\alpha\}$, corresponding too a given macrostates.
    The different microstates occur with probability $p_\alpha$.
    When wen no longer have exact knowledge of the microstate of a
    system the system is said to be in a \textit{mixed state}.
    The ensemble average of the quantum mechanical expectation
    value is given by:
    \begin{align}
        \label{eq:quantum_macrostates_ensemble_avg}
        \bar{\braket{O}} &= 
            \sum_\alpha p_\alpha \braket{\psi_\alpha|O|\psi_\alpha}
            = \sum_{\alpha,m,n} p_\alpha
                \braket{\psi_\alpha|m}\braket{m|O|n}\braket{n|\psi_\alpha}
                \nonumber\\
            &= \sum_{m,n} \braket{n|\rho|m}\braket{m|O|n}
                = \text{tr}(\rho O)
    \end{align}
    where we have introduced the density matrix:
    \begin{defi}[Density Matrix]
        The density matrix $\rho(t)$ is defined as
        \begin{align}
            \label{eq:density_matrix_def}
        \braket{n|\rho(t)|m} \equiv
        \sum_\alpha p_\alpha \braket{n|\psi_\alpha} \braket{\psi_\alpha|m}
        \end{align}
        or
        \begin{align}
            \label{eq:density_matrix_def_2}
            \rho(t) \equiv \sum_\alpha p_\alpha
                \ket{\psi_\alpha}\bra{\psi_\alpha}
        \end{align}
    \end{defi}
    Density matrix is denoted by $\rho(t)$ by analogy of the notation for
    P.D.F, since $\rho$ often represents density.
    
    Density matrix satisfies several good properties:
    \begin{itemize}
        \item Normalized
        \item Hermiticity
        \item Positivity. For any $\Phi$, $\braket{\Phi|\rho|\Phi} \geq 0$.
    \end{itemize}
    The time evolution of density matrix, directly obtained from Schrodinger's
    equation, is
    \begin{align}
        \label{eq:quantum_macrostates:density_matrix:evolution}
        i\hbar \frac{\partial}{\partial t}\rho = [H,\rho]
    \end{align}

    \subsection{The Mathematical Theory of Communication}
    \label{sec:The_Mathematical_Theory_of_Communication}
    \subsubsection{Introduction}

    \paragraph{What is information}
    In this part, it is implied that \textit{information} in this work
    does not carry the usual sense in people's daily life. The semantic
    aspect of a message is considered to be irrelavant, for purpose
    of generality of the design of communication systems.

    \paragraph{Measure of information} 
    % TODO Read the paper of Hartley to understand what is information:
    % Hartley, R. V. L., "Transmission of Information," Bell System Technical
    % Journal, July 1928, p. 535

    Then it refers to a paper by Hartley to substantiate the use of
    \begin{align}
        \label{eq:measure_of_information}
        S = \log(M)
    \end{align}
    as a measure of information. More specifically, we assume we have a
    set of possible messages. Then $M$ is the cardinality of this set.
    Then $S$ is a measure of the information produced when one message
    is chosen from the set. Once again, we regard all choices 
    being equally possible.

    Note that the base of logrithm in \ref{eq:measure_of_information}
    is undefined. Choosing a base constituting choosing a unit of
    the measure. Two such measures, when calculated in different units,
    are related by a simple constant.

    Conventionally, a base $2$ is chosen. The resulted unit is called
    bits. If the base $10$ is chosen, then the units may be called
    decimal digits. If the base $e$ is chosen, then the units is
    called natural units.

    Also, the author lists several points to illustrate the
    convenience of this measure.

    \paragraph{Communication systems}
    Next the author defines the necessary components of a \textit{
    communication system}, and categorizes it into discrete systems,
    continuous systems and mixed systems.

    \subsubsection{Discrete Noiseless Systems}

    \paragraph{Discrete Noiseless Channel}
    \label{sec:Discrete Noiseless Channel}
    This part deals with another measure, the measure of the capacity
    of a channel to transmit information. It defines the capacity of a
    discrete channel as:
    \begin{align}
        \label{eq:capacity_of_disc_chan}
        C\equiv \lim_{T\to \infty} \frac{\log N(T)}{T}
    \end{align}
    where $N(T)$ is the number of allowed signals of duration $T$.
    Several examples are given with formula for $C$ in each particular
    example.
    
    \paragraph{Discrete Source of Information}
    Next, it proceeds to discuss the statistical property of the source
    of information. Pointing out that a statistical knowledge of the
    source of information can help peopole craft special protocals to
    reduce the required capacity of the channel, the article gradually
    focuses on the statistical property of sourcs. It profess that while a
    discrete source could be represented by a statistical source, a
    statistical process can also be considered a discrete source. The
    second claim is substatiated by several exmaples.

    In one example of natural language, the article defines a 
    \textit{n-gram} case to produce natural language from statistical
    information.

    \paragraph{Series of Approximations to English}
    As the title suggests, this part illustrate two serial levels of
    steps to approximate the English language using statistical knowledge
    of appearance of alphabets (the first method) and words (the second
    example). The article claims that "a sufficiently complex stochastic 
    process will give a satisfactory representation of a discrete source".
    Although I am largely against this juvenile view.

    \paragraph{Graphical Representation of a Markoff Process}
    Then the article mentions a graphical way to represent the aforementioned
    approximation process, and gives three examples on page 46.

    \paragraph{Ergodic and Mixed Sources}
    Now the article comes to a special type of stochastic process, ergodic
    processes. A rough idea of "ergodic" is given in page 45. The idea is
    so important that I felt compeled to present it here:

    "In an ergodic process every sequence produced by
    the process is the same in statistical properties. Thus the letter
    frequencies, digram frequencies, etc., obtained from particular
    sequences, will, as the lengths of the sequences increase, approach
    definite limits independent of the particular sequence.
    Actually
    this is not true of every sequence but the set for which it is false
    has probability zero. Roughly the ergodic property means 
    \textbf{statistical homogeneity}."

    Next, the article claims that artificial languages given in previous
    examples are ergodic, because the corresponding graph does not
    have two properties: they does not comprise two or more \textit{isolated
    parts}, and they $gcd$ of the lengths of all \textit{circuits }is one.
    The precise meaning is listed in page 47. Roughly, an analogy made by
    myself helps to
    catch the points. If we picture an stochastic process as a connected 
    area, then isolated parts are its connected components, whereas the
    circuit are the recurrent pattterns.

    Naturally , a stochastic process may exhibit a mixed behavir, in which
    there are several different sources $L_1,L_2,L_3,\cdots$, which are
    each of homogeneous, i.e. ergodic, statistical structure. This is
    discussed following the introduction of ergodicity in page 48.

    Then the article declare that except in special cases, ergodicity is
    always assumed. This purpose is analogous to that of in statistical
    physics, to "identify averages along a sequence with averages over 
    the ensemble of possible sequences", with "the probability of a
    discrepancy being zero".

    Lastly, the article mentions a fact regarding the equilibrium of
    the system. A process is called stationary, if it satisfies a
    equilibrium condition:
    \begin{align}
        \label{eq:Ergodic_and_Mixed_States:equil_condition}
        P_j = \sum_i P_i \cdot P_i(j)
    \end{align}
    where $P_j$ is the probability of being in state $j$, and $P_i(j)$ is
    the transition probability from $i$ to $j$.
    The fact is that ergodic process is, in a sense, always stationary.

    \paragraph{The Entropy of an Information Source}
    This part first defines the entropy of a discrete source of finite state
    to be:
    \begin{align}
        \label{eq:entropy_of_discrete_infor_source}
        H \equiv& \sum_i P_i H_i\\
        =& -\sum_{i,j} P_i p_i(j) \log(p_i(j)) \nonumber
    \end{align}
    It is "the entropy of the source per symbol of text". Another
    definition for entropy per second is also listed.

    Following this definition are some theorems, which I consider to be
    the most essential and influential part of the whole book (although
    I have not yet read the whole book). They are:

    \begin{thm}[Theorem 3 on page 55]
        Given any $\epsilon >0$ and $\delta >0$, we can find an $N_0$
        such that the sequences of any legnth $N\geq N_0$ fall into
        two class:
        \begin{itemize}
            \item A set whose total probability is less than $\epsilon$.
            \item The remainder, all of whose members have probabilities
                satisfying the inequality
                \begin{align}
                    \label{eq:Entropy_of_info_source:thm3}
                    \left| \frac{\log p^{-1}}{N}-H\right| < \delta
                \end{align}
        \end{itemize}
    \end{thm}
    "In other words we are almost certain to have 
    $\frac{\log p^{-1}}{N}$ very close to $H$ when $N$ is large."
    For me, this reads quite like the \textit{second law of
    thermaldynamics}.

    % TODO having trouble understanding the Theorem 4 on page 55.

\section{Anchor}
% This is just an anchor for seperating thebibliography from above contents.
\begin{thebibliography}{1}
	\bibitem{Sakurai} Sakurai, J. J. Modern Quantum Mechanics, Addison Wesley.
	\bibitem{BSCS} Bosonization and Strongly Correlated Systems. Cambridge.

        \href{http://www.cambridge.org/us/academic/subjects/physics/condensed-matter-physics-nanoscience-and-mesoscopic-physics/bosonization-and-strongly-correlated-systems}{Cambridge Press Link}

    \bibitem{DETA} Martin Braun. Differential Equations and Their
    Applications. 4ed. Springer.
    \bibitem{Kardar} M. Kardar. Statistical Physics of Particles (2007)
\end{thebibliography}
\section{License}
The entire content of this work (including the source code
for TeX files and the generated PDF documents) by 
Hongxiang Chen (nicknamed we.taper, or just Taper) is
licensed under a 
\href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative 
Commons Attribution-NonCommercial-ShareAlike 4.0 International 
License}. Permissions beyond the scope of this 
license may be available at \url{mailto:we.taper[at]gmail[dot]com}.

\end{document}
