\subsection{\texorpdfstring{$\int_0^\infty \frac{\sin(x)}{x}$}{}}
From \href{http://math.stackexchange.com/questions/5248/solving-the-integral-int-0-infty-frac-sinxx-dx-frac-pi2}{Math.SE}. 
By \href{http://math.stackexchange.com/users/1102/aryabhata}{Aryabhata}.

I believe this can also be solved using double integrals.

It is possible (if I remember correctly) to justify switching the order of integration to give the equality:

$$\int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dy \Bigg)\, dx = \int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dx \Bigg)\,dy$$
Notice that
$$\int_{0}^{\infty} e^{-xy} \sin x\,dy = \frac{\sin x}{x}$$

This leads us to

$$\int_{0}^{\infty} \Big(\frac{\sin x}{x} \Big) \,dx = \int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dx \Bigg)\,dy$$
Now the right hand side can be found easily, using integration by parts.

\begin{align*}
I &= \int e^{-xy} \sin x \,dx = -e^{-xy}{\cos x} - y \int e^{-xy} \cos x \, dx\\
&= -e^{-xy}{\cos x} - y \Big(e^{-xy}\sin x + y \int e^{-xy} \sin x \,dx \Big)\\
&= \frac{-ye^{-xy}\sin x - e^{-xy}\cos x}{1+y^2}.
\end{align*}
Thus $$\int_{0}^{\infty} e^{-xy} \sin x \,dx = \frac{1}{1+y^2}$$
Thus $$\int_{0}^{\infty} \Big(\frac{\sin x}{x} \Big) \,dx = \int_{0}^{\infty}\frac{1}{1+y^2}\,dy = \frac{\pi}{2}.$$

\subsection{Bayes' theorem}
\label{sec:Bayes-theorem}
I am quite unfamiliar with this formula, so I decided to make a note
here for future reference.

This formula starts from the definition of conditional probability:
\begin{align}
P(A|B)\equiv \frac{P(A\&B)}{P(B)}
\end{align}
this definition if quite intuitively pleasing if $P(B)$ is multiplied
to the left side. Then one easily deduce that:
\begin{thm}[Bayes' theorem] \begin{align}
   	P(A|B)=\frac{P(B|A)P(A)}{P(B)}
   	\end{align} \end{thm}

We have also an extended form. Suppose the event space is partitioned
into $\{A_i\}$, then we have (also easily proved):
\begin{align}
P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_j P(B|A_j)P(A_j)}
\end{align}
Here's a good reading \href{http://plato.stanford.edu/entries/bayes-theorem/}{Bayes' Theorem in SEP}.

\subsection{System of Differential Equations}
This is a smsall note of \cite{DETA}.

pp. 266.

\begin{defi}
   	$\mathbf{x(t)}$ is a vector whose elements are $x_i(t)$.
   	$ \frac{d}{d t}$ acts on vector $\mathbf{x}$ element-wise.
   	$\dot{\mathbf{x}}$ is abbrevation for $\frac{d}{d t}\mathbf{x}$
\end{defi}

pp. 291.

\begin{thm}[Existence-uniqueness theorem]
   	There exists one, and only one, solution of the initial-value
   	problem
   	
   	\begin{align}
   	\dot{\mathbf{x}}=\mathbf{A}\mathbf{x}\text{, }&
   	\mathbf{x}(t_0) = \mathbf{x}^0 = 
   	\left(
   	\begin{array}{c}
   	x^0_1\\
   	x^0_2\\
   	\cdots
   	\end{array} 
   	\right)
   	\end{align}
   	
   	Moreover, this solution exists for $-\infty\langle t\langle \infty$.
\end{thm}
\begin{remark}
   	By this, any non-trivial solution $\mathbf{x}(t)\neq 0$ at any
   	time $t$. Also notice that the elements of $\mathbf{A}$ are just
   	numbers.
\end{remark}

\begin{thm}
   	The dimension of the space $\mathbf{V}$ of all solutions of the
   	homogeneous linear system of differential equations:
   	\begin{align}
   	\frac{d\mathbf{x}}{dt}=\mathbf{Ax}
   	\end{align}
   	is $n$, i.e. the dimension of vector $\mathbf{x}$.
\end{thm} 
\subsection{ODE by Arnold}
sec. 14
\begin{defi}
   	\begin{align}
   	\label{eq:e^A}
   	e^A &= I + A + \frac{A^2}{2!} + \frac{A^3}{3!}\\
   	\text{or}& \nonumber \\
   	e^A &= \lim_{n\to \infty}(I+\frac{A}{n})^n
   	\end{align}
   	where $I$ is the identity matrix.
\end{defi}
Equivalance of the two definition will be addressed in the Theorem on
pp. 165.

Important theorems:
\begin{thm}[pp. 158]
   	The series $e^A$ converges for any $A$ uniformly on each set
   	$X=\{A:||A||\leq a\}$, $a\in \mathbb{R}$.
\end{thm}
\begin{thm}[pp. 160]
   	$$e^{At} = H^t$$
   	where $H^t$ is the translation operator which sends every polynomial
   	$p(x)$ into $p(x+t)$.
\end{thm}
\begin{thm}[pp. 163]
   	$$\frac{d}{dt} e^{tA} = Ae^{tA}$$
\end{thm}
\begin{thm}[Fundamental Theorem of the Theory of Linear Equations with
   	Constant Coefficients]
   	The solution of:
   	\begin{align}
   	\label{eq:fund_thm_of_linear_eqs_const_coef}
   	\dot{\mathbf{x}} = A\mathbf{x}
   	\end{align}
   	with initial condition $\phi(0) = \mathbf{x}_0$ is
   	\begin{align}
   	\mathbf{\phi}(t) = e^{tA}\mathbf{x}_0
   	\end{align}
\end{thm}

Practically solution to
$$ \dot{\mathbf{x}} = A\mathbf{x}$$
(pp. 173, Sec 17)
(Assuming $A$ is diagonalizable.)
\begin{itemize}
   	\item Find the eigenvectors $\xi_1,\cdots ,\xi_n$ and eigenvalues
   	$\lambda_1,\cdots ,\lambda_n$. Use them as basis.
   	\item Expand the initial condition in the new basis.
   	\begin{align}
   	\mathbf{x}_0=\sum_{k=1}^{n} C_k\xi_k
   	\end{align}
   	\item Then $\phi(t) = \sum_{k=1}^n C_k e^{\lambda_k t}\xi_k$
\end{itemize}

\subsection{Why 0/0 is undefined?}
If we suppose
$$ \frac{0}{0}= \triangle $$
Consider the following derivation:
\begin{align}
\frac{0}{0} \cdot 1 &= \triangle \cdot 1 = \triangle \\
0 \cdot \frac{1}{0} &= \triangle\\
\Rightarrow \triangle &= 0
\end{align}
This is already bad enough. And we are forced to define $\frac{1}{0}$.
Let $\frac{1}{0} = \square$, which literally means $1=0\cdot \square = 0$.
This is disastrous.

Alternatively, we could let
\begin{enumerate}
   	\item Let $\frac{1}{0}$ be undefined.
   	\item Or let $\frac{1}{0} =
   	\infty$.
   	\item Or, let $\frac{a}{b}\cdot c= a\cdot \frac{c}{b}$ be not 
   	true when $b=0$.
\end{enumerate}
The third idea is disastrous for algebraic manipulation. \footnote{
   	Or more speicifically, it is a disaster for field theory.
}
The first idea is not good. Since defining $\frac{1}{0}=\infty$ turns
out to be very useful in both mathematics and physics. Actully, in
physics it is common practice to set $\frac{a}{0}=\pm\infty$ for
any nonzero number $a$, where the sign of $\infty$ is determined by 
the sign of $a$.
The second idea is okey. But then we are faced with a serious problem.
We have to define $\triangle \equiv 0 \cdot \infty$

$\triangle \cdot 2 = \triangle$, What will be of $\triangle + 1$?

\subsection{Gamma Function and Stirling's formula with Stationary phase
approximation}

The Gamma Function for $z\in\C$ with a positive real part is

\begin{equation}
    \Gamma(z+1) = \int_0^\infty \dd{x} x^z e^{-x}
\end{equation}

It has an approximation formula called the Stirling formula. Here I obtain the
Stirling formula with the Stationary phase approximation in pp.108 of
\cite{Altland2010}. 

Without further restricting $z$ into $\R$, we can use Stationary phase approximation
to get the result.\footnote{
    If we have restricted $z\in\R$, then instead of Stationary phase
approximation, we would be using Saddle point approximation.}
Note that since $\Re(z)>0$, the natural logarithm $\ln(z)$ is well
defined, so we have

\begin{equation}
    \int_0^\infty \dd{x} x^z e^{-x} = 
    \int_0^\infty \dd{x} e^{\ln(x^z)} e^{-x} = 
    \int_0^\infty \dd{x} e^{\ln(x^z)-x} 
\end{equation}

Let $F(x,z) = -(\ln(x^z)-x)$, then $\frac{dF}{dx}(x_0)=-\frac{z}{x_0}+1=0$ gives the
saddle point $x_0=z$. And $\frac{\dd[2]{F}}{\dd x^2}(x_0)=z/x_0^2=1/z$
confirms that this is really a minimum point. Therefore, the integration is
largely determined by the value around point $x_0=z$. Expanding $F$ around
$x_0=z$, we have
\begin{equation}
    F(x_0 + y)\approx F(x_0=z) + \frac{1}{2z} y^2 
    = z-\ln(z^z) + \frac{1}{2z}y^2
\end{equation}
Therefore
\begin{align}
    \int_0^\infty \dd{x} e^{\ln(x^z)-x} &= 
    \int_0^\infty \dd{x} e^{-F(x,z)} \approx
    \int_{-z}^\infty \dd{y} e^{-z+\ln(z^z) - \frac{1}{2z}y^2}
\end{align}

Now as $z\to\infty$, we have
\begin{align}
    \int_0^\infty \dd{x} e^{\ln(x^z)-x}
    &\approx e^{-z+\ln(z^z)} \int_{-\infty}^\infty \dd{y} e^{- \frac{1}{2z}y^2} 
    = e^{z(\ln(z)-1)} \sqrt{2\pi z} \nonumber\\
    &= e^{\ln(z^z)}e^{-z}\sqrt{2\pi z}
    = (\frac{z}{e})^z \sqrt{2\pi z}
\end{align}
Thus we have
\begin{equation}
    \Gamma(z+1) \overset{z\to\infty}{\approx} e^{z(\ln(z)-1)} \sqrt{2\pi z}
    = (\frac{z}{e})^z \sqrt{2\pi z}
\end{equation}
For $z\in\C$, $\Re(z)>0$.
\subsection{Function satisfying \texorpdfstring{$f(x+y)=f(x)f(x)$}{}}
\label{sec:Function satisfying \texorpdfstring{$f(x+y)=f(x)f(x)$}{}}

Here I prove a simple fact that:
\begin{thm}
    Any smooth $f$ function of $\mathbb{R}$. If $f$ satisfy the
    relation:
    \begin{equation}
        f(x+y)=f(x)f(y)
    \end{equation}
    for any $x,y\in \mathbb{R}$, and $f(0)\neq 0$. Then:
    \begin{equation}
        f(x) = e^{k x}
    \end{equation}
    where $k=\dv{f}{x}\left(0\right)$.
\end{thm}
\begin{proof}
    Notice that since $f(0)=f(0+0)=f(0)^2$, and $f(0)\neq 0$, we have
    $f(0)=1$.

    Consider
    \begin{align*}
        \dv{f}{x} = \lim_{\Delta x\to 0} 
        \frac{f(x+\Delta x)-f(x)}{\Delta x}
            = \lim_{\Delta x\to 0} 
            \frac{f(x)\left(f(\Delta x)-1\right)}{\Delta x}
    \end{align*}
    Using Taylor expansion about $f(0)$ and notice $f(0)=1$, one will
    easily get
    \begin{equation*}
        \lim_{\Delta x\to 0} 
            \frac{f(\Delta x)-1}{\Delta x}
            = f'(0)
    \end{equation*}
    Therefore, 
    \begin{equation*}
        \dv{f}{x} = f'(0)f(x)
    \end{equation*}
    Hence by thoey of differential equations (I don't quite remember
    the specific theorem), one has $f(x)=e^{f'(0)x}$.
\end{proof}
    
\subsection{Dirac delta function}

It is quite troublesome to deal with this function $\delta(x)$. For example, sometimes I
have to deal with $\frac{\dd}{\dd{x}}\delta(x)$, or $\delta'(x)$. Here I present an argument, not
proof, of the result.

I treat $\delta(x)$ not as a function, but as an operator. Therefore, we have to
use a test function $f(x)$ to see what $\delta'(x)$ really is. Assuming that the
dirac delta function still follows the rule of \textit{integration by parts}.
Assuming our test function vanishes, or at least does not go to infinity when
$x\to\infty$ or $x\to-\infty$. Then
\begin{align}
    \int_{-\infty}^\infty \dd{x} \delta'(x) f(x) &=
    \eval{\delta(x)f(x)}_{-\infty}^\infty - \int\dd{x}\delta(x)\frac{\dd}{\dd x}f(x)
    = 0 - \dv{f}{x}(0)
\end{align}
Therefore, we say that in this situation,
\begin{equation}
    \delta'(x) = -\delta(x)\dv{x}
\end{equation}

But there is one pitfall that confuses me for a long time. Because:
\begin{align*}
    \int\dd{x}g(x)x\delta'(x) = -\int\dd{x}(g(x)+xg'(x))\delta(x)
    = -\int\dd{x}g(x)\delta(x)
\end{align*}
This lures me into believing
\begin{equation}
    x\delta'(x) = -\delta(x)
    \label{eq:xdp--d}
\end{equation}
Hence
\begin{equation}
    \delta'(x) = -\frac{\delta(x)}{x}
    \label{eq:dp--dx}
\end{equation}
While \ref{eq:xdp--d} is correct, \ref{eq:dp--dx} is of course wrong.

\textbf{Note}: This, in my opinion, is an argument against completely treating
$\delta(x)$ as a function.
\subsection{A beautiful product for sine function}

We have:
\begin{equation}
    \sin(x) =  x\prod_{n=1}^\infty \left(1-\frac{x^2}{n^2\pi^2}\right)
\end{equation}
Obviously the left and right has the same roots. But the proof of equality needs
some work. See related links:
\begin{enumerate}
    \item  
        \href{http://math.stackexchange.com/questions/674769/sinx-infinite-product-formula-how-did-euler-prove-it}{Phy.SE:
        $sin(x)$ infinite product formula: how did Euler prove it?},
    \item \href{https://en.wikipedia.org/wiki/Basel_problem}{Wikipedia-Basel Problem}.
\end{enumerate}

This formula is also used in calculating a path integral, see pp.114 of
\cite{Altland2010}.

\subsection{Functional Derivative}

The following is an answer to a question posted on Physics.StackExchange. It
contains a good definition of functional derivative and a specific example. The
link to that post is
\url{http://physics.stackexchange.com/questions/314638/variational-derivative-of-function-with-respect-to-its-derivative}.

The question asks:
\begin{myquote} \enquote{
    What is 
    $$ \frac{\delta f(t)}{\delta \dot{f}(t)} $$
    Where $\dot{f}(t)=df/dt$.
} \end{myquote}

The best answer is:
\begin{myquote} \enquote{
    The definition of the functional derivative of a functional $I[g]$ is the
    distribution $\frac{\delta I}{\delta g}(\tau)$ such that 
    $$\left\langle \frac{\delta I}{\delta g}, h\right\rangle 
        := \frac{d}{d\alpha}\bigg\rvert_{\alpha=0} I[g+ \alpha h]$$
    for every test function $h$. In our case, assuming to deal with functions which
    suitably vanish before reaching $\pm \infty$,
    $$I[g] = \int_{-\infty}^t g(x)dx$$
    so that
    $$I[\dot{f}]= f(t)$$
    as requested. Going on with the procedure
    $$\left\langle \frac{\delta I}{\delta g}, h\right\rangle 
        = \frac{d}{d\alpha}|_{\alpha=0}  \int_{-\infty}^t(g(\tau)+ \alpha h(\tau)) d\tau 
        = \int_{-\infty}^t h(\tau) dx 
        = \int_{-\infty}^{+\infty} \theta(t-\tau)h(\tau) d\tau
    $$
    where $\theta(\tau)=1$ for $\tau\geq 0$ and $\theta(\tau)=0$ for $\tau<0$
    and so 
$$\frac{\delta f(t)}{\delta \dot{f}}(\tau) = \frac{\delta I}{\delta g}(\tau)= \theta(t-\tau)$$
} \end{myquote}
In my opinion, the answer has assumed a inner product 
$$\left\langle f,g \right\rangle := \int f(t)g(t)\dd{t}$$.

\subsection{A centered coordinate making two variables into one}

This technique is found in p.126 of \cite{Altland2010}. It should be helpful in
a much broader context.

The original equation is:
\begin{equation}
    \partial^2 \phi = \partial_\phi V(\phi)
\end{equation}
with $\partial^2 = \partial^2_\tau + \partial^2_x$. We propose a change of
coordinate $r\equiv \sqrt{x^2+\tau^2}$. Then after careful calculation, one
finds:
\begin{align*}
    \partial_x &= \pm\frac{\sqrt{r^2-\tau^2}}{r}\partial_r \\
    \partial^2_x &= 
        \frac{1}{r^2}\left(\frac{\tau^2}{r}\partial_r +
        (r^2-\tau^2)\partial^2_r\right) 
\end{align*}
and similarly for $\partial_\tau$. They combine into:
\begin{equation}
    \partial^2 = 
        \frac{1}{r^2}\left(\frac{x^2+\tau^2}{r}\partial_r +
        (r^2-\tau^2+r^2-x^2)\partial^2_r\right)
    = \frac{1}{r}\partial_r + \partial^2_r
\end{equation}
Therefore, we have a new equation:
\begin{equation}
    \partial^2_r \phi = - \frac{1}{r}\partial_r\phi + \partial_\phi V(\phi)
\end{equation}
with a "friction force" of the form $-\frac{1}{r}\partial_r\phi$.
\subsection{Common Pitfalls}
\label{sec:Common Pitfalls}
    \subsubsection{Tensors}
    \label{sec:Tensors}
    
    The $k$-th component $\psi_k$ of the transpose of $(A\phi)^i = A^i_k
    \phi^k$ is still $A^i_k \phi^k$, although we have to be careful about
    the position of the index.

    \subsubsection{Transpose}
    \label{sec:Transpose}
    
    In matrix theory, if we ahave the following structure:
    $$\braket{x,y} := x y$$
    here $x,y$ are regarded as two column vectors in vector space $V$.
    Then the transpose of a linear transformation $A:V\to V$ is defined
    as
    \begin{defi}[Transpose]
    \nomenclature{Transpose}{\nomrefpage.}
        $\tensor[^t]{A}{}:V^* \to V^*$ is such that:
        \begin{equation}
            \braket{x,Ay} = \braket{\tensor[^t]{A}{}x,y}
        \end{equation}
        for any $x,y\in V$.
    \end{defi}

    In my notation, it maps $x^n\to A^m_n x^m$, or $x_n\to A^m_n x_m$.
    (My notation is non-standard, but I think it expresses the idea).


\subsection{Matrix Operations}
    \subsubsection{Matrix Exponentials}
    \label{sec:Matrix-Exponentials}
    \paragraph{The Zassenhaus formula}
    I found the formula from Wikipedia page:
    \href{https://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula}{Baker–Campbell–Hausdorff formula}.
    \begin{align}
        e^{t(X+Y)} &= e^{tX}~ e^{tY} ~e^{-\frac{t^2}{2} [X,Y]} ~
        e^{\frac{t^3}{6}(2[Y,[X,Y]]+ [X,[X,Y]] )} \nonumber\\
        & \times
          e^{\frac{-t^4}{24}([[[X,Y],X],X] + 3[[[X,Y],X],Y] + 3[[[X,Y],Y],Y]) }
        \times\cdots
    \end{align}

    \paragraph{Logarithm of Matrix}
    The logarithm of  a matrix $A$ is defined when there is a matrix $B$ such that
    $e^B=A$, then $\ln(A)=B$.

    Using this, we have a formula to calculate the determinant of a some special
    matrix.

    Let $A = I + B$, and suppose $\det(A)\neq
    0$. Then, we have:
    \begin{equation}
        \det(A) = \exp(\tr(\ln(A)))
    \end{equation}
    Since $\exp{\sum_i \ln(\lambda_i)} = \Pi_i \exp(\ln(\lambda_i))=\Pi_i
    \lambda_i=\det(A)$.

    Next, we recap several series:
    \begin{align*}
        \ln(1+x) &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}x^n \\
        \exp(x) &= \sum_{n=0}^{\infty} \frac{x^n}{n!}
    \end{align*}
    Then we start the calculation:
    \begin{align*}
        \ln(I+B) &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}B^n \\
        \tr(\ln(I+B)) &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\tr(B^n) \\
        \det(A) &= \exp(\tr(\ln(I+B))) 
            = \exp(\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\tr(B^n)) \\
            &\approx 1+ \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\tr(B^n) \\
            &\approx 1+ \tr(B)
    \end{align*}

    Therefore, we have, for appropriate matrix $B$:
    \begin{thm}
    \begin{equation}
        \det(A) \equiv \det(I+B) \approx 1+\tr(B)
    \end{equation}
    \end{thm}
    
    A typical situation is that $A$ is the transformation matrix:
    \begin{equation}
        A^\mu_\nu = \frac{\partial x'_\mu}{\partial x_\nu} 
        = \delta^\mu_\nu + \text{(first order matrix $B$)}^\mu_\nu
    \end{equation}
