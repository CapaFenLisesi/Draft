\subsection{\texorpdfstring{$\int_0^\infty \frac{\sin(x)}{x}$}{}}
From \href{http://math.stackexchange.com/questions/5248/solving-the-integral-int-0-infty-frac-sinxx-dx-frac-pi2}{Math.SE}. 
By \href{http://math.stackexchange.com/users/1102/aryabhata}{Aryabhata}.

I believe this can also be solved using double integrals.

It is possible (if I remember correctly) to justify switching the order of integration to give the equality:

$$\int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dy \Bigg)\, dx = \int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dx \Bigg)\,dy$$
Notice that
$$\int_{0}^{\infty} e^{-xy} \sin x\,dy = \frac{\sin x}{x}$$

This leads us to

$$\int_{0}^{\infty} \Big(\frac{\sin x}{x} \Big) \,dx = \int_{0}^{\infty} \Bigg(\int_{0}^{\infty} e^{-xy} \sin x \,dx \Bigg)\,dy$$
Now the right hand side can be found easily, using integration by parts.

\begin{align*}
I &= \int e^{-xy} \sin x \,dx = -e^{-xy}{\cos x} - y \int e^{-xy} \cos x \, dx\\
&= -e^{-xy}{\cos x} - y \Big(e^{-xy}\sin x + y \int e^{-xy} \sin x \,dx \Big)\\
&= \frac{-ye^{-xy}\sin x - e^{-xy}\cos x}{1+y^2}.
\end{align*}
Thus $$\int_{0}^{\infty} e^{-xy} \sin x \,dx = \frac{1}{1+y^2}$$
Thus $$\int_{0}^{\infty} \Big(\frac{\sin x}{x} \Big) \,dx = \int_{0}^{\infty}\frac{1}{1+y^2}\,dy = \frac{\pi}{2}.$$

\subsection{Bayes' theorem}
\label{sec:Bayes-theorem}
I am quite unfamiliar with this formula, so I decided to make a note
here for future reference.

This formula starts from the definition of conditional probability:
\begin{align}
P(A|B)\equiv \frac{P(A\&B)}{P(B)}
\end{align}
this definition if quite intuitively pleasing if $P(B)$ is multiplied
to the left side. Then one easily deduce that:
\begin{thm}[Bayes' theorem] \begin{align}
   	P(A|B)=\frac{P(B|A)P(A)}{P(B)}
   	\end{align} \end{thm}

We have also an extended form. Suppose the event space is partitioned
into $\{A_i\}$, then we have (also easily proved):
\begin{align}
P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_j P(B|A_j)P(A_j)}
\end{align}
Here's a good reading \href{http://plato.stanford.edu/entries/bayes-theorem/}{Bayes' Theorem in SEP}.

\subsection{System of Differential Equations}
This is a smsall note of \cite{DETA}.

pp. 266.

\begin{defi}
   	$\mathbf{x(t)}$ is a vector whose elements are $x_i(t)$.
   	$ \frac{d}{d t}$ acts on vector $\mathbf{x}$ element-wise.
   	$\dot{\mathbf{x}}$ is abbrevation for $\frac{d}{d t}\mathbf{x}$
\end{defi}

pp. 291.

\begin{thm}[Existence-uniqueness theorem]
   	There exists one, and only one, solution of the initial-value
   	problem
   	
   	\begin{align}
   	\dot{\mathbf{x}}=\mathbf{A}\mathbf{x}\text{, }&
   	\mathbf{x}(t_0) = \mathbf{x}^0 = 
   	\left(
   	\begin{array}{c}
   	x^0_1\\
   	x^0_2\\
   	\cdots
   	\end{array} 
   	\right)
   	\end{align}
   	
   	Moreover, this solution exists for $-\infty\langle t\langle \infty$.
\end{thm}
\begin{remark}
   	By this, any non-trivial solution $\mathbf{x}(t)\neq 0$ at any
   	time $t$. Also notice that the elements of $\mathbf{A}$ are just
   	numbers.
\end{remark}

\begin{thm}
   	The dimension of the space $\mathbf{V}$ of all solutions of the
   	homogeneous linear system of differential equations:
   	\begin{align}
   	\frac{d\mathbf{x}}{dt}=\mathbf{Ax}
   	\end{align}
   	is $n$, i.e. the dimension of vector $\mathbf{x}$.
\end{thm} 
\subsection{ODE by Arnold}
sec. 14
\begin{defi}
   	\begin{align}
   	\label{eq:e^A}
   	e^A &= I + A + \frac{A^2}{2!} + \frac{A^3}{3!}\\
   	\text{or}& \nonumber \\
   	e^A &= \lim_{n\to \infty}(I+\frac{A}{n})^n
   	\end{align}
   	where $I$ is the identity matrix.
\end{defi}
Equivalance of the two definition will be addressed in the Theorem on
pp. 165.

Important theorems:
\begin{thm}[pp. 158]
   	The series $e^A$ converges for any $A$ uniformly on each set
   	$X=\{A:||A||\leq a\}$, $a\in \mathbb{R}$.
\end{thm}
\begin{thm}[pp. 160]
   	$$e^{At} = H^t$$
   	where $H^t$ is the translation operator which sends every polynomial
   	$p(x)$ into $p(x+t)$.
\end{thm}
\begin{thm}[pp. 163]
   	$$\frac{d}{dt} e^{tA} = Ae^{tA}$$
\end{thm}
\begin{thm}[Fundamental Theorem of the Theory of Linear Equations with
   	Constant Coefficients]
   	The solution of:
   	\begin{align}
   	\label{eq:fund_thm_of_linear_eqs_const_coef}
   	\dot{\mathbf{x}} = A\mathbf{x}
   	\end{align}
   	with initial condition $\phi(0) = \mathbf{x}_0$ is
   	\begin{align}
   	\mathbf{\phi}(t) = e^{tA}\mathbf{x}_0
   	\end{align}
\end{thm}

Practically solution to
$$ \dot{\mathbf{x}} = A\mathbf{x}$$
(pp. 173, Sec 17)
(Assuming $A$ is diagonalizable.)
\begin{itemize}
   	\item Find the eigenvectors $\xi_1,\cdots ,\xi_n$ and eigenvalues
   	$\lambda_1,\cdots ,\lambda_n$. Use them as basis.
   	\item Expand the initial condition in the new basis.
   	\begin{align}
   	\mathbf{x}_0=\sum_{k=1}^{n} C_k\xi_k
   	\end{align}
   	\item Then $\phi(t) = \sum_{k=1}^n C_k e^{\lambda_k t}\xi_k$
\end{itemize}

\subsection{Why 0/0 is undefined?}
If we suppose
$$ \frac{0}{0}= \triangle $$
Consider the following derivation:
\begin{align}
\frac{0}{0} \cdot 1 &= \triangle \cdot 1 = \triangle \\
0 \cdot \frac{1}{0} &= \triangle\\
\Rightarrow \triangle &= 0
\end{align}
This is already bad enough. And we are forced to define $\frac{1}{0}$.
Let $\frac{1}{0} = \square$, which literally means $1=0\cdot \square = 0$.
This is disastrous.

Alternatively, we could let
\begin{enumerate}
   	\item Let $\frac{1}{0}$ be undefined.
   	\item Or let $\frac{1}{0} =
   	\infty$.
   	\item Or, let $\frac{a}{b}\cdot c= a\cdot \frac{c}{b}$ be not 
   	true when $b=0$.
\end{enumerate}
The third idea is disastrous for algebraic manipulation. \footnote{
   	Or more speicifically, it is a disaster for field theory.
}
The first idea is not good. Since defining $\frac{1}{0}=\infty$ turns
out to be very useful in both mathematics and physics. Actully, in
physics it is common practice to set $\frac{a}{0}=\pm\infty$ for
any nonzero number $a$, where the sign of $\infty$ is determined by 
the sign of $a$.
The second idea is okey. But then we are faced with a serious problem.
We have to define $\triangle \equiv 0 \cdot \infty$

$\triangle \cdot 2 = \triangle$, What will be of $\triangle + 1$?
\subsection{Funcation satisfying \texorpdfstring{$f(x+y)=f(x)f(x)$}{}}
\label{sec:Funcation satisfying \texorpdfstring{$f(x+y)=f(x)f(x)$}{}}

Here I prove a simple fact that:
\begin{thm}
    Any smooth $f$ function of $\mathbb{R}$. If $f$ satisfy the
    relation:
    \begin{equation}
        f(x+y)=f(x)f(y)
    \end{equation}
    for any $x,y\in \mathbb{R}$, and $f(0)\neq 0$. Then:
    \begin{equation}
        f(x) = e^{k x}
    \end{equation}
    where $k=\dv{f}{x}\left(0\right)$.
\end{thm}
\begin{proof}
    Notice that since $f(0)=f(0+0)=f(0)^2$, and $f(0)\neq 0$, we have
    $f(0)=1$.

    Consider
    \begin{align*}
        \dv{f}{x} = \lim_{\Delta x\to 0} 
        \frac{f(x+\Delta x)-f(x)}{\Delta x}
            = \lim_{\Delta x\to 0} 
            \frac{f(x)\left(f(\Delta x)-1\right)}{\Delta x}
    \end{align*}
    Using Taylor expansion about $f(0)$ and notice $f(0)=1$, one will
    easily get
    \begin{equation*}
        \lim_{\Delta x\to 0} 
            \frac{f(\Delta x)-1}{\Delta x}
            = f'(0)
    \end{equation*}
    Therefore, 
    \begin{equation*}
        \dv{f}{x} = f'(0)f(x)
    \end{equation*}
    Hence by thoey of differential equations (I don't quite remember
    the specific theorem), one has $f(x)=e^{f'(0)x}$.
\end{proof}
    
\subsection{Common Pitfalls}
\label{sec:Common Pitfalls}
    \subsubsection{Tensors}
    \label{sec:Tensors}
    
    The $k$-th component $\psi_k$ of the transpose of $(A\phi)^i = A^i_k
    \phi^k$ is still $A^i_k \phi^k$, although we have to be careful about
    the position of the index.

    \subsubsection{Transpose}
    \label{sec:Transpose}
    
    In matrix theory, if we ahave the following structure:
    $$\braket{x,y} := x y$$
    here $x,y$ are regarded as two column vectors in vector space $V$.
    Then the transpose of a linear transformation $A:V\to V$ is defined
    as
    \begin{defi}[Transpose]
    \nomenclature{Transpose}{\nomrefpage.}
        $\tensor[^t]{A}{}:V^* \to V^*$ is such that:
        \begin{equation}
            \braket{x,Ay} = \braket{\tensor[^t]{A}{}x,y}
        \end{equation}
        for any $x,y\in V$.
    \end{defi}

    In my notation, it maps $x^n\to A^m_n x^m$, or $x_n\to A^m_n x_m$.
    (My notation is non-standard, but I think it expresses the idea).


\subsection{Matrix Operations}
    \subsubsection{Matrix Exponentials}
    \label{sec:Matrix-Exponentials}
    I found these formulae from Wikipedia page:
    \href{https://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula}{Baker–Campbell–Hausdorff formula}.
    \paragraph{The Zassenhaus formula}
    \begin{align}
        e^{t(X+Y)} &= e^{tX}~ e^{tY} ~e^{-\frac{t^2}{2} [X,Y]} ~
        e^{\frac{t^3}{6}(2[Y,[X,Y]]+ [X,[X,Y]] )} \nonumber\\
        & \times
          e^{\frac{-t^4}{24}([[[X,Y],X],X] + 3[[[X,Y],X],Y] + 3[[[X,Y],Y],Y]) }
        \times\cdots
    \end{align}

    \paragraph{Logarithm of Matrix}
    The logarithm of  a matrix $A$ is defined when there is a matrix $B$ such that
    $e^B=A$, then $\ln(A)=B$.

    Using this, we have a formula to calculate the determinant of a some special
    matrix.

    Let $A = I + B$, and suppose $\det(A)\neq
    0$. Then, we have:
    \begin{equation}
        \det(A) = \exp(\tr(\ln(A)))
    \end{equation}
    Since $\exp{\sum_i \ln(\lambda_i)} = \Pi_i \exp(\ln(\lambda_i))=\Pi_i
    \lambda_i=\det(A)$.

    Next, we recap several series:
    \begin{align*}
        \ln(1+x) &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}x^n \\
        \exp(x) &= \sum_{n=0}^{\infty} \frac{x^n}{n!}
    \end{align*}
    Then we start the calculation:
    \begin{align*}
        \ln(I+B) &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}B^n \\
        \tr(\ln(I+B)) &= \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\tr(B^n) \\
        \det(A) &= \exp(\tr(\ln(I+B))) 
            = \exp(\sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\tr(B^n)) \\
            &\approx 1+ \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n}\tr(B^n) \\
            &\approx 1+ \tr(B)
    \end{align*}

    Therefore, we have, for appropriate matrix $B$:
    \begin{thm}
    \begin{equation}
        \det(A) \equiv \det(I+B) \approx 1+\tr(B)
    \end{equation}
    \end{thm}
    
    A typical situation is that $A$ is the transformation matrix:
    \begin{equation}
        A^\mu_\nu = \frac{\partial x'_\mu}{\partial x_\nu} 
        = \delta^\mu_\nu + \text{(first order matrix $B$)}^\mu_\nu
    \end{equation}
